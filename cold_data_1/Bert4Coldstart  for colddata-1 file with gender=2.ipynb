{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62490640",
   "metadata": {},
   "source": [
    "# Bert4ColdStart using cold_data_03.csv dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb6c8b",
   "metadata": {},
   "source": [
    "# Import needed Libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785564da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-26 14:39:15.637152: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-26 14:39:16.448112: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-26 14:39:16.448199: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-26 14:39:16.448207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import joblib\n",
    "import pickle\n",
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "tqdm.pandas()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Set to the index of the GPU you want to use\n",
    "torch.cuda.set_per_process_memory_fraction(0.8)  # Limit memory usage to 80%\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "#Ignore warning\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7132de",
   "metadata": {},
   "source": [
    "### Checks whether a GPU is available for use with PyTorch or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf01f5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109d6cc",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "## here we are setting all the needed setting to make any changes in the project from one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9955671",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'sbr_sample_size':10000000, #the selected sample size from SBR file\n",
    "    'seed': 0,\n",
    "    'task_name': '',\n",
    "    'task_num': 4,\n",
    "    'dataset_path': '',\n",
    "    'pretrain_path': '',\n",
    "    'source_path': 'sbr_data_1M.csv',\n",
    "    'target_path': 'cold_data_1.csv',\n",
    "    'train_batch_size': 1024,\n",
    "    'val_batch_size': 1024,\n",
    "    'test_batch_size': 1024,\n",
    "    'sample': 'random',\n",
    "    'negsample_savefolder': './data/neg_data/',\n",
    "    'negsample_size': 99,\n",
    "    'max_len': 20,\n",
    "    'item_min': 5,\n",
    "    'save_path': './checkpoint/',\n",
    "    'task': -1,\n",
    "    'valid_rate': 100,\n",
    "    'model_name': 'BERT_ColdstartModel',\n",
    "    'epochs': 20,\n",
    "    're_epochs': 20,\n",
    "    'lr': 0.0005,\n",
    "    'device': 'cuda',\n",
    "    'is_parallel': False,\n",
    "    'local_rank': None,\n",
    "    'num_gpu': 1,\n",
    "    'weight_decay': 0.0,\n",
    "    'decay_step': 5,\n",
    "    'gamma': 0.5,\n",
    "    'num_users': 1,\n",
    "    'num_items': 1,\n",
    "    'num_embedding': 1,\n",
    "    'num_labels': 1,\n",
    "    'k': 20,\n",
    "    'metric_ks': [5, 20],\n",
    "    'best_metric': 'NDCG@10',\n",
    "    'hidden_size': 128,\n",
    "    'block_num': 2,\n",
    "    'num_groups': 4,\n",
    "    'num_heads': 4,\n",
    "    'dropout': 0.3,\n",
    "    'bert_mask_prob': 0.3,\n",
    "    'factor_num': 128,\n",
    "    'embedding_size': 128,\n",
    "    'dilations': [1, 4],\n",
    "    'kernel_size': 3,\n",
    "    'is_mp': False,\n",
    "    'pad_token': 0,\n",
    "    'temp': 7,\n",
    "    'l2_emb': 0.0,\n",
    "    'mtl_task_num': 1,\n",
    "    'test_method': 'ufo',\n",
    "    'val_method': 'ufo',\n",
    "    'test_size': 0.1,\n",
    "    'val_size': 0.1111,\n",
    "    'cand_num': 100,\n",
    "    'sample_method': 'high-pop',\n",
    "    'sample_ratio': 0.3,\n",
    "    'num_ng': 4,\n",
    "    'loss_type': 'BPR',\n",
    "    'init_method': 'default',\n",
    "    'optimizer': 'default',\n",
    "    'early_stop': True,\n",
    "    'reg_1': 0.0,\n",
    "    'reg_2': 0.0,\n",
    "    'context_window': 2,\n",
    "    'rho': 0.5,\n",
    "    'node_dropout': 0.1,\n",
    "    'mess_dropout': 0.1,\n",
    "    'hidden_size_list': [128, 128],\n",
    "    'latent_dim': 128,\n",
    "    'anneal_cap': 0.2,\n",
    "    'total_anneal_steps': 1000,\n",
    "    'kd': False,\n",
    "    'alpha': 0.4,\n",
    "    'add_num_times': 2,\n",
    "    'is_pretrain': 1,\n",
    "    'user_profile': 'gender',\n",
    "    'prun_rate': 0,\n",
    "    'll_max_itemnum': 0,\n",
    "    'lifelong_eval': True,\n",
    "    'task1_out': 0,\n",
    "    'task2_out': 0,\n",
    "    'task3_out': 0,\n",
    "    'task4_out': 0,\n",
    "    'eval': True,\n",
    "    'ch':True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7072b82",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f763875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select sample from the SBR file sbr_data_1M.csv as it's to large to be computed\n",
    "\n",
    "def generate_sbr_sample():\n",
    "    df = pd.read_csv(\"sbr_data_1M.csv\")\n",
    "    sampled_df = df.sample(args['sbr_sample_size'])\n",
    "    print('sample size : ', sampled_df.count())\n",
    "    file_name = \"sbr_sample_\"+str(args['sbr_sample_size'])+\".csv\"\n",
    "    args['source_path'] = file_name #update the source path with the generated sampled SBR\n",
    "    sampled_df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76089edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Item ID Resetter with Label Encoding\n",
    "class item_reset_df(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"=\" * 10, \"Initialize Reset DataFrame Object\", \"=\" * 10)\n",
    "        self.item_enc = LabelEncoder()\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "        print(\"=\" * 10, \"Resetting item ids in DataFrame\", \"=\" * 10)\n",
    "        df['item_id'] = self.item_enc.fit_transform(df['item_id']) + 1\n",
    "        return df\n",
    "\n",
    "    def inverse_transform(self, df):\n",
    "        df['item_id'] = self.item_enc.inverse_transform(df['item_id']) - 1\n",
    "        return df\n",
    "    \n",
    "#Construct training data\n",
    "def construct_data(args, item_min):\n",
    "    path1 = args['target_path']\n",
    "    path2 = args['source_path']\n",
    "    df1 = pd.read_csv(path1, usecols=['user_id', 'item_id', 'click','gender'])\n",
    "    df1=df1[df1['gender']==2]\n",
    "    df1 = df1[df1.click.isin([1])]\n",
    "    df2 = pd.read_csv(path2, usecols=['user_id', 'item_id', 'click','gender'])\n",
    "    df2 = df2[df2.click.isin([1])]\n",
    "    user_counts = df2.groupby('user_id').size()\n",
    "    user_subset = np.in1d(df2.user_id, user_counts[user_counts >= item_min].index)\n",
    "    df2 = df2[user_subset].reset_index(drop=True)\n",
    "\n",
    "    assert (df2.groupby('user_id').size() < item_min).sum() == 0\n",
    "    s_item_count = len(set(df2['item_id']))\n",
    "    reset_ob = cold_reset_df()\n",
    "    df2, df1 = reset_ob.fit_transform(df2, df1)\n",
    "    user1 = set(df1.user_id.values.tolist())\n",
    "    user2 = set(df2.user_id.values.tolist())\n",
    "    user = user1 & user2\n",
    "    df1 = df1[df1.user_id.isin(list(user))]\n",
    "    df2 = df2[df2.user_id.isin(list(user))]\n",
    "    new_data1 = []\n",
    "    new_data2 = []\n",
    "    for u in user:\n",
    "        tmp_data2 = df2[df2.user_id == u][:-3].values.tolist()\n",
    "        tmp_data1 = df1[df1.user_id == u].values.tolist()\n",
    "        new_data1.extend(tmp_data1)\n",
    "        new_data2.extend(tmp_data2)\n",
    "        \n",
    "    new_data1 = pd.DataFrame(new_data1, columns=df1.columns)\n",
    "    new_data2 = pd.DataFrame(new_data2, columns=df2.columns)\n",
    "    user_count = len(set(new_data1.user_id.values.tolist()))\n",
    "    reset_item = item_reset_df()\n",
    "    new_data1 = reset_item.fit_transform(new_data1)\n",
    "    t_item_count = len(set(new_data1['item_id']))\n",
    "    return new_data1, new_data2, user_count, t_item_count, s_item_count\n",
    "\n",
    "#Construct cold-start dataset\n",
    "def colddataset(item_min, args, path=None):\n",
    "    target_data, source_data, user_count, t_item_count, s_item_count = construct_data(args, item_min)\n",
    "    print(\"+++user_history+++\")\n",
    "    user_history = source_data.groupby('user_id').item_id.apply(list).to_dict()\n",
    "    target = target_data.groupby('user_id').item_id.apply(list).to_dict()\n",
    "\n",
    "    examples = []\n",
    "    for u, t_list in tqdm(target.items()):\n",
    "        for t in t_list:\n",
    "            e_list = [user_history[u] + [0], t]\n",
    "            examples.append(e_list)\n",
    "    examples = pd.DataFrame(examples, columns=['source', 'target'])\n",
    "    return examples, user_count, s_item_count, t_item_count\n",
    "\n",
    "\n",
    "class cold_reset_df(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"=\" * 10, \"Initialize Reset DataFrame Object\", \"=\" * 10)\n",
    "        self.item_enc1 = LabelEncoder()\n",
    "        self.item_enc2 = LabelEncoder()\n",
    "        self.user_enc = LabelEncoder()\n",
    "\n",
    "    def fit_transform(self, df1, df2):\n",
    "        print(\"=\" * 10, \"Resetting user ids and item ids in DataFrame\", \"=\" * 10)\n",
    "        df = pd.concat([df1['user_id'], df2['user_id']])\n",
    "        df = self.user_enc.fit_transform(df) + 1\n",
    "        df1['item_id'] = self.item_enc1.fit_transform(df1['item_id']) + 1\n",
    "        df1['user_id'] = df[:len(df1)]\n",
    "        df2['item_id'] = self.item_enc2.fit_transform(df2['item_id']) + 1\n",
    "        df2['user_id'] = df[len(df1):]\n",
    "        return df1, df2\n",
    "\n",
    "    def inverse_transform(self, df):\n",
    "        df['item_id'] = self.item_enc.inverse_transform(df['item_id']) - 1\n",
    "        df['user_id'] = self.user_enc.inverse_transform(df['user_id']) - 1\n",
    "        return df\n",
    "\n",
    "class ColdDataset(data_utils.Dataset):\n",
    "    def __init__(self, x, y, max_len, mask_token):\n",
    "        self.seqs = x\n",
    "        self.targets = y\n",
    "        self.max_len = max_len\n",
    "        self.mask_token = mask_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = self.seqs[index]\n",
    "        target = self.targets[index]\n",
    "        seq = seq[-self.max_len:]\n",
    "        seq_len = len(seq)\n",
    "        seq_mask_len = self.max_len - seq_len\n",
    "        seq = [0] * seq_mask_len + seq\n",
    "        return torch.LongTensor(seq), torch.LongTensor([target])\n",
    "\n",
    "class ColdEvalDataset(data_utils.Dataset):\n",
    "    def __init__(self, x, y, max_len, mask_token, num_item):\n",
    "        self.seqs = x\n",
    "        self.targets = y\n",
    "        self.max_len = max_len\n",
    "        self.mask_token = mask_token\n",
    "        self.num_item = num_item + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = self.seqs[index]\n",
    "        target = self.targets[index]\n",
    "        labels = [0] * self.num_item\n",
    "        labels[target] = 1\n",
    "        seq = seq[-self.max_len:]\n",
    "        seq_len = len(seq)\n",
    "        seq_mask_len = self.max_len - seq_len\n",
    "        seq = [self.mask_token] * seq_mask_len + seq\n",
    "        return torch.LongTensor(seq), torch.LongTensor(labels)\n",
    "\n",
    "# create and return a PyTorch data loader for a training dataset\n",
    "def get_train_loader(dataset, args):\n",
    "    if args['is_parallel']:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['train_batch_size'], sampler=DistributedSampler(dataset))\n",
    "    else:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['train_batch_size'], shuffle=True, pin_memory=True)\n",
    "    return dataloader\n",
    "\n",
    "# create and return a PyTorch data loader for a validation dataset\n",
    "def get_val_loader(dataset, args):\n",
    "    if args['is_parallel']:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['val_batch_size'], sampler=DistributedSampler(dataset))\n",
    "    else:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['val_batch_size'], shuffle=False, pin_memory=True)\n",
    "    return dataloader\n",
    "\n",
    "# create and return a PyTorch data loader for a testing dataset\n",
    "def get_test_loader(dataset, args):\n",
    "    if args['is_parallel']:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['test_batch_size'], sampler=DistributedSampler(dataset))\n",
    "    else:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['test_batch_size'], shuffle=False, pin_memory=True)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82176a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation and Splitting Function for Cold Start Recommendation System\n",
    "def get_data(args):\n",
    "    path = args['dataset_path']\n",
    "    rng = random.Random(args['seed'])\n",
    "    data, user_count, vocab_size, item_count = colddataset(args['item_min'], args)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data.source.values.tolist(),\n",
    "                                                            data.target.values.tolist(),\n",
    "                                                            test_size=0.2, random_state=args['seed'])\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=args['seed'])\n",
    "    # not hot and cold#####\n",
    "\n",
    "    args['num_users'] = user_count\n",
    "    args['num_items'] = item_count\n",
    "    args['num_embedding'] = vocab_size\n",
    "\n",
    "    train_dataset, valid_dataset = ColdDataset(x_train, y_train, args['max_len'], args['pad_token']), ColdEvalDataset(\n",
    "        x_val, y_val, args['max_len'], args['pad_token'], args['num_items'])\n",
    "    test_dataset = ColdEvalDataset(x_test, y_test, args['max_len'], args['pad_token'], args['num_items'])\n",
    "    train_dataloader = get_train_loader(train_dataset, args)\n",
    "    valid_dataloader = get_val_loader(valid_dataset, args)\n",
    "    test_dataloader = get_test_loader(test_dataset, args)\n",
    "    return train_dataloader, valid_dataloader, test_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48e5aee",
   "metadata": {},
   "source": [
    "# Bert4ColdStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e7d8ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "                 / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper Section 3.4, last paragraph notice that BERT used the GELU instead of RELU\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        return self.pe.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size=512):\n",
    "        super().__init__(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Embedding which is consisted with under features\n",
    "        1. TokenEmbedding : normal embedding matrix\n",
    "        2. PositionalEmbedding : adding positional information using sin, cos\n",
    "        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n",
    "\n",
    "        sum of all these features are output of BERTEmbedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, max_len, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: total vocab size\n",
    "        :param embed_size: embedding size of token embedding\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(max_len=max_len, d_model=embed_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        x = self.token(sequence) + self.position(sequence)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional Encoder = Transformer (self-attention)\n",
    "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        \"\"\"\n",
    "        :param hidden: hidden size of transformer\n",
    "        :param attn_heads: head sizes of multi-head attention\n",
    "        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class bottle_net(nn.Module):\n",
    "    def __init__(self, hidden):\n",
    "        super(bottle_net, self).__init__()\n",
    "        self.hidden = hidden\n",
    "        self.hidden_size = int(hidden / 4)\n",
    "        self.linear1 = nn.Linear(self.hidden, self.hidden_size)\n",
    "        self.linear2 = nn.Linear(self.hidden_size, self.hidden)\n",
    "        self.ln = nn.LayerNorm(self.hidden, eps=1e-8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = F.leaky_relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.ln(out)\n",
    "        return out\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        max_len =   args['max_len']\n",
    "        n_layers =  args['block_num']\n",
    "        heads = args['num_heads']\n",
    "        vocab_size = args['num_embedding'] + 1\n",
    "        hidden = args['hidden_size'] \n",
    "        self.hidden = hidden\n",
    "        self.is_mp = args['is_mp']\n",
    "        dropout = args['dropout']\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=self.hidden, max_len=max_len, dropout=dropout)\n",
    "\n",
    "        transformer_blocks = []\n",
    "        for _ in range(n_layers):\n",
    "            transformer_blocks.append(TransformerBlock(hidden, heads, hidden * 4, dropout))\n",
    "            if self.is_mp:\n",
    "                transformer_blocks.append(bottle_net(self.hidden))\n",
    "        self.transformer_blocks = nn.ModuleList(transformer_blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x)\n",
    "        # running over multiple transformer blocks\n",
    "        for i, block in enumerate(self.transformer_blocks):\n",
    "            mp_input = x\n",
    "            if i % 2 == 0:\n",
    "                x = block.forward(x, mask)\n",
    "            else:\n",
    "                if self.is_mp:\n",
    "                    mp_out = block(mp_input)\n",
    "                    x = mp_out + x\n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        pass\n",
    "\n",
    "class BERT_ColdstartModel(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.bert = BERT(args)\n",
    "        self.num_items = args['num_items']\n",
    "        self.out = nn.Linear(self.bert.hidden*args['max_len'], args['num_items'] + 1)#update\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bert(x)\n",
    "        \n",
    "        return self.out(x.view(-1,self.bert.hidden*args['max_len']))#update\n",
    "\n",
    "    def predict(self, x, item):\n",
    "        x = self.bert(x)\n",
    "        item_emb = self.bert.embedding.token(item)\n",
    "        logits = x.matmul(item_emb.transpose(1, 2))\n",
    "        print('shape logits')\n",
    "#         print(logits.shape)\n",
    "        logits = logits.mean(1)\n",
    "#         print(logits.shape)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "281b9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generic function get the configured model in args\n",
    "def get_model(args, linear_feature_columns=None, dnn_feature_columns=None, history_feature_list=None):\n",
    "    if args['model_name'] == 'BERT_ColdstartModel':\n",
    "        return BERT_ColdstartModel(args)\n",
    "    elif args['model_name'] == 'Peter4Coldstart':\n",
    "        return Peter4Coldstart(args)\n",
    "    \n",
    "    # elif name == 'vae':\n",
    "    #     return VAECF(args)\n",
    "    # elif name == 'item2vec':\n",
    "    #     return Item2Vec(args)\n",
    "    else:\n",
    "        raise ValueError('unknown model name: ' +  args['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f55442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, re=True):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    if re:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    else:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92addc69",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bc0521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform validation on a recommendation model.\n",
    "def Sequence_full_Validate(epoch, model, dataloader, writer, args, test=False):\n",
    "    print(\"+\" * 20, \"Valid Epoch {}\".format(epoch + 1), \"+\" * 20)\n",
    "    model.eval()\n",
    "    avg_metrics = {}\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        tqdm_dataloader = dataloader\n",
    "        for data in tqdm_dataloader:\n",
    "            data = [x.to(args['device']) for x in data]\n",
    "            seqs, labels = data\n",
    "            if test:\n",
    "                scores = model.predict(seqs)\n",
    "            else:\n",
    "                scores = model(seqs)\n",
    "#             scores = scores.mean(1) #update\n",
    "#             print('score')\n",
    "#             print(scores.shape)\n",
    "#             print(labels.shape)\n",
    "            metrics = recalls_and_ndcgs_for_ks(scores, labels, args['metric_ks'], args)\n",
    "            i += 1\n",
    "            for key, value in metrics.items():\n",
    "                if key not in avg_metrics:\n",
    "                    avg_metrics[key] = value\n",
    "                else:\n",
    "                    avg_metrics[key] += value\n",
    "    for key, value in avg_metrics.items():\n",
    "        avg_metrics[key] = value / i\n",
    "    print(avg_metrics)\n",
    "    for k in sorted(args['metric_ks'], reverse=True):\n",
    "        writer.add_scalar('Train/NDCG@{}'.format(k), avg_metrics['NDCG@%d' % k], epoch)\n",
    "    return avg_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ce84fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the recall and NDCG@ks metrics \n",
    "def recalls_and_ndcgs_for_ks(scores, labels, ks, args):\n",
    "    metrics = {}\n",
    "\n",
    "    answer_count = labels.sum(1)\n",
    "    answer_count_float = answer_count.float()\n",
    "    labels_float = labels.float()\n",
    "#     print(scores.shape)\n",
    "    rank = (-scores).argsort(dim=0)\n",
    "    cut = rank\n",
    "    for k in sorted(ks, reverse=True):\n",
    "#        print(cut.shape)\n",
    "       cut = cut[:, :k]\n",
    "#        print(labels_float.shape)\n",
    "       hits = torch.gather(labels_float,1,cut)\n",
    "#         labels_float.gather( cut,0)\n",
    "       metrics['Recall@%d' % k] = (hits.sum(1) / answer_count_float).mean().item()\n",
    "\n",
    "       position = torch.arange(2, 2+k)\n",
    "       weights = 1 / torch.log2(position.float()).to(args['device'])\n",
    "#        print(weights.shape)\n",
    "#        print(hits.shape) \n",
    "       dcg = (hits * weights).sum(1)\n",
    "       idcg = torch.Tensor([weights[:min(n, k)].sum() for n in answer_count]).to(args['device'])\n",
    "       ndcg = (dcg / idcg).mean()\n",
    "       metrics['NDCG@%d' % k] = ndcg\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49544397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a sequence-based recommendation model for one epoc\n",
    "def SequenceTrainer(epoch, model, dataloader, optimizer, writer, args): #schedular,\n",
    "    print(\"+\" * 20, \"Train Epoch {}\".format(epoch + 1), \"+\" * 20)\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        data = [x.to(args['device']) for x in data]\n",
    "        seqs, labels = data\n",
    "#         print('data shape',labels.shape)\n",
    "        logits = model(seqs) # B x T x V\n",
    "#         print('pred shape',logits.shape)\n",
    "        if 'cold' in args['task_name'] or ('life_long' in args['task_name'] and args['task'] != 0):\n",
    "            logits = logits.mean(1)\n",
    "            labels = labels.view(-1)\n",
    "        else:\n",
    "            logits = logits.view(-1, logits.size(-1)) # (B*T) x V\n",
    "            labels = labels.view(-1)  # B*T\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.detach().cpu().item()\n",
    "    writer.add_scalar('Train/loss', running_loss / len(dataloader), epoch)\n",
    "    print(\"Training CE Loss: {:.5f}\".format(running_loss / len(dataloader)))\n",
    "    return optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ae33851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SeqTrain(epochs, model, train_loader, val_loader, writer, args):\n",
    "    if args['is_pretrain'] == 0:\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                     lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "    model = model.to(args['device'])\n",
    "    if args['is_parallel']:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model,  find_unused_parameters=True,device_ids=[args['local_rank']], output_device=[args['local_rank']])\n",
    "    best_metric = 0\n",
    "    all_time = 0\n",
    "    val_all_time = 0\n",
    "    for epoch in range(epochs):\n",
    "        since = time.time()\n",
    "        optimizer = SequenceTrainer(epoch, model, train_loader, optimizer, writer, args)\n",
    "        tmp = time.time() - since\n",
    "        print('one epoch train:', tmp)\n",
    "        all_time += tmp\n",
    "        val_since = time.time()\n",
    "        metrics = Sequence_full_Validate(epoch, model, val_loader, writer, args)\n",
    "        val_tmp = time.time() - val_since\n",
    "        print('one epoch val:', val_tmp)\n",
    "        val_all_time += val_tmp\n",
    "        if args['is_pretrain'] == 0 and 'acc' in args['task_name']:\n",
    "            if metrics['NDCG@20'] >= 0.0193:\n",
    "                break\n",
    "        i = 1\n",
    "        current_metric = metrics['NDCG@5']\n",
    "        if best_metric <= current_metric:\n",
    "            best_metric = current_metric\n",
    "            best_model = deepcopy(model)\n",
    "            state_dict = model.state_dict()\n",
    "            if 'life' in args['task_name']:\n",
    "                torch.save(state_dict, os.path.join(args['save_path'],\n",
    "                                                         '{}_{}_seed{}_task_{}_best_model.pth'.format('sequence',\n",
    "                                                                                                      args['model_name'],\n",
    "                                                                                                      args['seed'],\n",
    "                                                                                                      args['task'])))\n",
    "            else:\n",
    "                torch.save(state_dict, os.path.join(args['save_path'], '{}_{}_seed{}_is_pretrain_{}_best_model_lr{}_wd{}_block{}_hd{}_emb{}.pth'.format(args['task_name'], args['model_name'], args['seed'], args['is_pretrain'],\n",
    "                                                                                                                              args['lr'], args['weight_decay'], args['block_num'], args['hidden_size'], args['embedding_size'])))\n",
    "        else:\n",
    "            i += 1\n",
    "            if i == 10:\n",
    "                print('early stop!')\n",
    "                break\n",
    "    print('train_time:', all_time)\n",
    "    print('val_time:', val_all_time)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119ddcaf",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d635c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============cold_start=============\n",
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting user ids and item ids in DataFrame ==========\n",
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting item ids in DataFrame ==========\n",
      "+++user_history+++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22225/22225 [00:00<00:00, 81439.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate sample from SBR file\n",
      "sample size :  user_id           10000000\n",
      "item_id           10000000\n",
      "click             10000000\n",
      "follow            10000000\n",
      "like              10000000\n",
      "share             10000000\n",
      "video_category    10000000\n",
      "watching_times    10000000\n",
      "gender            10000000\n",
      "age               10000000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if args['is_parallel']:\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    torch.cuda.set_device(args['local_rank'])\n",
    "device = torch.device(args['device'])\n",
    "set_seed(args['seed'])\n",
    "writer = SummaryWriter()\n",
    "print('=============cold_start=============')\n",
    "train_loader, val_loader, test_loader = get_data(args) #, user_noclick\n",
    "\n",
    "print('generate sample from SBR file')\n",
    "generate_sbr_sample()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da61d9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrain\n",
      "++++++++++++++++++++ Train Epoch 1 ++++++++++++++++++++\n",
      "Training CE Loss: 10.82987\n",
      "one epoch train: 3.6641407012939453\n",
      "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0013950892857142857, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.00027901785714285713, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 11.489222764968872\n",
      "++++++++++++++++++++ Train Epoch 2 ++++++++++++++++++++\n",
      "Training CE Loss: 8.57306\n",
      "one epoch train: 3.524775981903076\n",
      "++++++++++++++++++++ Valid Epoch 2 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0018067070549087866, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n",
      "one epoch val: 11.413374185562134\n",
      "++++++++++++++++++++ Train Epoch 3 ++++++++++++++++++++\n",
      "Training CE Loss: 6.76013\n",
      "one epoch train: 3.5471553802490234\n",
      "++++++++++++++++++++ Valid Epoch 3 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0013950892857142857, 'NDCG@20': tensor(0.0007, device='cuda:0'), 'Recall@5': 0.0008370535714285714, 'NDCG@5': tensor(0.0005, device='cuda:0')}\n",
      "one epoch val: 11.519253253936768\n",
      "++++++++++++++++++++ Train Epoch 4 ++++++++++++++++++++\n",
      "Training CE Loss: 5.12816\n",
      "one epoch train: 3.5678317546844482\n",
      "++++++++++++++++++++ Valid Epoch 4 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0012555803571428572, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.00027901785714285713, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 11.499012470245361\n",
      "++++++++++++++++++++ Train Epoch 5 ++++++++++++++++++++\n",
      "Training CE Loss: 4.12555\n",
      "one epoch train: 3.7867748737335205\n",
      "++++++++++++++++++++ Valid Epoch 5 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0015207801812461444, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.00013950892857142856, 'NDCG@5': tensor(6.9754e-05, device='cuda:0')}\n",
      "one epoch val: 11.494152545928955\n",
      "++++++++++++++++++++ Train Epoch 6 ++++++++++++++++++++\n",
      "Training CE Loss: 3.62084\n",
      "one epoch train: 3.540905475616455\n",
      "++++++++++++++++++++ Valid Epoch 6 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0025042516977659295, 'NDCG@20': tensor(0.0008, device='cuda:0'), 'Recall@5': 0.0006975446428571429, 'NDCG@5': tensor(0.0004, device='cuda:0')}\n",
      "one epoch val: 11.531051635742188\n",
      "++++++++++++++++++++ Train Epoch 7 ++++++++++++++++++++\n",
      "Training CE Loss: 3.33390\n",
      "one epoch train: 3.554924488067627\n",
      "++++++++++++++++++++ Valid Epoch 7 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0013881802691945008, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.00013950892857142856, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 11.920926332473755\n",
      "++++++++++++++++++++ Train Epoch 8 ++++++++++++++++++++\n",
      "Training CE Loss: 3.13844\n",
      "one epoch train: 3.5484118461608887\n",
      "++++++++++++++++++++ Valid Epoch 8 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0013881802691945008, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.00013950892857142856, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 11.651176929473877\n",
      "++++++++++++++++++++ Train Epoch 9 ++++++++++++++++++++\n",
      "Training CE Loss: 2.99690\n",
      "one epoch train: 3.5598795413970947\n",
      "++++++++++++++++++++ Valid Epoch 9 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0012555803571428572, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.0004185267857142857, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 11.806335210800171\n",
      "++++++++++++++++++++ Train Epoch 10 ++++++++++++++++++++\n",
      "Training CE Loss: 2.89528\n",
      "one epoch train: 3.5446577072143555\n",
      "++++++++++++++++++++ Valid Epoch 10 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0011091624120516436, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.00027901785714285713, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 11.403390407562256\n",
      "++++++++++++++++++++ Train Epoch 11 ++++++++++++++++++++\n",
      "Training CE Loss: 2.81239\n",
      "one epoch train: 3.5500245094299316\n",
      "++++++++++++++++++++ Valid Epoch 11 ++++++++++++++++++++\n",
      "{'Recall@20': 0.001946215983480215, 'NDCG@20': tensor(0.0008, device='cuda:0'), 'Recall@5': 0.00041161776919450076, 'NDCG@5': tensor(0.0003, device='cuda:0')}\n",
      "one epoch val: 11.745716571807861\n",
      "++++++++++++++++++++ Train Epoch 12 ++++++++++++++++++++\n",
      "Training CE Loss: 2.73949\n",
      "one epoch train: 3.5401604175567627\n",
      "++++++++++++++++++++ Valid Epoch 12 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0024973426812461446, 'NDCG@20': tensor(0.0009, device='cuda:0'), 'Recall@5': 0.0008370535714285714, 'NDCG@5': tensor(0.0004, device='cuda:0')}\n",
      "one epoch val: 11.549031257629395\n",
      "++++++++++++++++++++ Train Epoch 13 ++++++++++++++++++++\n",
      "Training CE Loss: 2.68463\n",
      "one epoch train: 3.8288543224334717\n",
      "++++++++++++++++++++ Valid Epoch 13 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0013950892857142857, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.0006975446428571429, 'NDCG@5': tensor(0.0003, device='cuda:0')}\n",
      "one epoch val: 11.448750495910645\n",
      "++++++++++++++++++++ Train Epoch 14 ++++++++++++++++++++\n",
      "Training CE Loss: 2.66110\n",
      "one epoch train: 3.5789794921875\n",
      "++++++++++++++++++++ Valid Epoch 14 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0018067070549087866, 'NDCG@20': tensor(0.0006, device='cuda:0'), 'Recall@5': 0.00027901785714285713, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 11.531433343887329\n",
      "++++++++++++++++++++ Train Epoch 15 ++++++++++++++++++++\n",
      "Training CE Loss: 2.60264\n",
      "one epoch train: 3.8639917373657227\n",
      "++++++++++++++++++++ Valid Epoch 15 ++++++++++++++++++++\n",
      "{'Recall@20': 0.002225233840623072, 'NDCG@20': tensor(0.0007, device='cuda:0'), 'Recall@5': 0.00027901785714285713, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 11.426352262496948\n",
      "++++++++++++++++++++ Train Epoch 16 ++++++++++++++++++++\n",
      "Training CE Loss: 2.58222\n",
      "one epoch train: 3.5557923316955566\n",
      "++++++++++++++++++++ Valid Epoch 16 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0009765625, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.0004185267857142857, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 11.72719120979309\n",
      "++++++++++++++++++++ Train Epoch 17 ++++++++++++++++++++\n",
      "Training CE Loss: 2.55368\n",
      "one epoch train: 3.556814193725586\n",
      "++++++++++++++++++++ Valid Epoch 17 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0015207801812461444, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.00013950892857142856, 'NDCG@5': tensor(8.8020e-05, device='cuda:0')}\n",
      "one epoch val: 11.550517559051514\n",
      "++++++++++++++++++++ Train Epoch 18 ++++++++++++++++++++\n",
      "Training CE Loss: 2.51662\n",
      "one epoch train: 3.855609655380249\n",
      "++++++++++++++++++++ Valid Epoch 18 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0023647427691945006, 'NDCG@20': tensor(0.0009, device='cuda:0'), 'Recall@5': 0.0006975446428571429, 'NDCG@5': tensor(0.0004, device='cuda:0')}\n",
      "one epoch val: 11.54628849029541\n",
      "++++++++++++++++++++ Train Epoch 19 ++++++++++++++++++++\n",
      "Training CE Loss: 2.49814\n",
      "one epoch train: 3.5739424228668213\n",
      "++++++++++++++++++++ Valid Epoch 19 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0011160714285714285, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.00013950892857142856, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 11.561851978302002\n",
      "++++++++++++++++++++ Train Epoch 20 ++++++++++++++++++++\n",
      "Training CE Loss: 2.47062\n",
      "one epoch train: 3.842409133911133\n",
      "++++++++++++++++++++ Valid Epoch 20 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0008370535714285714, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.00013950892857142856, 'NDCG@5': tensor(8.8020e-05, device='cuda:0')}\n",
      "one epoch val: 11.518653392791748\n",
      "train_time: 72.58603596687317\n",
      "val_time: 231.33368229866028\n",
      "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0023647427691945006, 'NDCG@20': tensor(0.0010, device='cuda:0'), 'Recall@5': 0.0009696534834802151, 'NDCG@5': tensor(0.0006, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "def process_training():\n",
    "    if args['is_pretrain'] == 1:\n",
    "        print(\"pretrain\")\n",
    "        model = get_model(args)\n",
    "        SeqTrain(args['epochs'], model, train_loader, val_loader, writer, args) #, user_noclick\n",
    "        writer.close()\n",
    "    elif args['is_pretrain'] == 2:\n",
    "        args['is_mp'] = False\n",
    "        model = get_model(args)\n",
    "        SeqTrain(args['epochs'], model, train_loader, val_loader, writer, args)\n",
    "        writer.close()\n",
    "    else:\n",
    "        print(\"transfer\")\n",
    "        best_weight = torch.load(args['pretrain_path'])\n",
    "        if 'peter' in args['model_name']:\n",
    "            args.is_mp = True\n",
    "            best_weight.pop('final_layer.weight')\n",
    "            best_weight.pop('final_layer.bias')\n",
    "        if 'bert' in args.model_name:\n",
    "            best_weight.pop('out.weight')\n",
    "            best_weight.pop('out.bias')\n",
    "\n",
    "        model = get_model(args)\n",
    "\n",
    "        model_state = model.module.state_dict() if args.is_parallel else model.state_dict()\n",
    "        best_weight = {k: v for k, v in best_weight.items() if k in model_state}\n",
    "        model_state.update(best_weight)\n",
    "        model.load_state_dict(model_state)\n",
    "            # if 'peter' in args.model_name:\n",
    "            #     for name, parm in model.named_parameters():\n",
    "            #         if 'rez' not in name and 'mp' not in name and 'final_layer' not in name:\n",
    "            #             parm.requires_grad = False\n",
    "            # else:\n",
    "            #     for name, parm in model.named_parameters():\n",
    "            #         if 'out' not in name:\n",
    "            #             parm.requires_grad = False\n",
    "        SeqTrain(args.epochs, model, train_loader, val_loader, writer, args)\n",
    "        writer.close()\n",
    "        \n",
    "    if args['eval']:\n",
    "        model = get_model(args)\n",
    "        best_weight = torch.load(os.path.join(args['save_path'],\n",
    "                                                  '{}_{}_seed{}_is_pretrain_{}_best_model_lr{}_wd{}_block{}_hd{}_emb{}.pth'.format(args['task_name'], args['model_name'], args['seed'], args['is_pretrain'],\n",
    "                                                                                                                                  args['lr'], args['weight_decay'], args['block_num'], args['hidden_size'], args['embedding_size'])))\n",
    "        model.load_state_dict(best_weight)\n",
    "        model = model.to(args['device'])\n",
    "        metrics = Sequence_full_Validate(0, model, test_loader, writer, args)\n",
    "\n",
    "        \n",
    "process_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e1509b",
   "metadata": {},
   "source": [
    "## Results (the best)\n",
    "### we can summarize the evaluation results as:\n",
    "\n",
    "#### Recall@20 : 0.0023647427691945006\n",
    "\n",
    "#### NDCG@20 : 0.0010\n",
    "\n",
    "#### Recall@5 : 0.0009696534834802151\n",
    "\n",
    "#### NDCG@5 : 0.0006\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a166b",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning - Trial 1\n",
    "\n",
    "## will perofrm changes in the Hyperparameter and check if the accuracy enhanced or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "656db306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Hyperparameter Tuning - cold_start =============\n",
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting user ids and item ids in DataFrame ==========\n",
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting item ids in DataFrame ==========\n",
      "+++user_history+++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15088/15088 [00:00<00:00, 51774.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate sample from SBR file\n",
      "sample size :  user_id           10000000\n",
      "item_id           10000000\n",
      "click             10000000\n",
      "follow            10000000\n",
      "like              10000000\n",
      "share             10000000\n",
      "video_category    10000000\n",
      "watching_times    10000000\n",
      "gender            10000000\n",
      "age               10000000\n",
      "dtype: int64\n",
      "pretrain\n",
      "++++++++++++++++++++ Train Epoch 1 ++++++++++++++++++++\n",
      "Training CE Loss: 10.63352\n",
      "one epoch train: 1.9018034934997559\n",
      "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0020287004765123127, 'NDCG@20': tensor(0.0006, device='cuda:0'), 'Recall@5': 0.0001953125, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 6.207072734832764\n",
      "++++++++++++++++++++ Train Epoch 2 ++++++++++++++++++++\n",
      "Training CE Loss: 9.33405\n",
      "one epoch train: 1.922003984451294\n",
      "++++++++++++++++++++ Valid Epoch 2 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0013671875, 'NDCG@20': tensor(0.0007, device='cuda:0'), 'Recall@5': 0.000390625, 'NDCG@5': tensor(0.0004, device='cuda:0')}\n",
      "one epoch val: 6.191406488418579\n",
      "++++++++++++++++++++ Train Epoch 3 ++++++++++++++++++++\n",
      "Training CE Loss: 8.56986\n",
      "one epoch train: 1.914088249206543\n",
      "++++++++++++++++++++ Valid Epoch 3 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0013230259530246258, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n",
      "one epoch val: 6.249755144119263\n",
      "++++++++++++++++++++ Train Epoch 4 ++++++++++++++++++++\n",
      "Training CE Loss: 7.84845\n",
      "one epoch train: 2.0981059074401855\n",
      "++++++++++++++++++++ Valid Epoch 4 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0040132394060492516, 'NDCG@20': tensor(0.0020, device='cuda:0'), 'Recall@5': 0.0017136509530246257, 'NDCG@5': tensor(0.0014, device='cuda:0')}\n",
      "one epoch val: 6.209808588027954\n",
      "++++++++++++++++++++ Train Epoch 5 ++++++++++++++++++++\n",
      "Training CE Loss: 7.11249\n",
      "one epoch train: 1.8925020694732666\n",
      "++++++++++++++++++++ Valid Epoch 5 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0032319894060492517, 'NDCG@20': tensor(0.0013, device='cuda:0'), 'Recall@5': 0.0006615129765123129, 'NDCG@5': tensor(0.0007, device='cuda:0')}\n",
      "one epoch val: 6.299800157546997\n",
      "++++++++++++++++++++ Train Epoch 6 ++++++++++++++++++++\n",
      "Training CE Loss: 6.38516\n",
      "one epoch train: 1.9019713401794434\n",
      "++++++++++++++++++++ Valid Epoch 6 ++++++++++++++++++++\n",
      "{'Recall@20': 0.002419325476512313, 'NDCG@20': tensor(0.0008, device='cuda:0'), 'Recall@5': 0.0005859375, 'NDCG@5': tensor(0.0003, device='cuda:0')}\n",
      "one epoch val: 6.248263597488403\n",
      "++++++++++++++++++++ Train Epoch 7 ++++++++++++++++++++\n",
      "Training CE Loss: 5.71298\n",
      "one epoch train: 1.8952405452728271\n",
      "++++++++++++++++++++ Valid Epoch 7 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0015625, 'NDCG@20': tensor(0.0006, device='cuda:0'), 'Recall@5': 0.0001953125, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 6.234760046005249\n",
      "++++++++++++++++++++ Train Epoch 8 ++++++++++++++++++++\n",
      "Training CE Loss: 5.12139\n",
      "one epoch train: 2.100992202758789\n",
      "++++++++++++++++++++ Valid Epoch 8 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0014427629765123128, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.0001953125, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 6.193563938140869\n",
      "++++++++++++++++++++ Train Epoch 9 ++++++++++++++++++++\n",
      "Training CE Loss: 4.58856\n",
      "one epoch train: 1.9025521278381348\n",
      "++++++++++++++++++++ Valid Epoch 9 ++++++++++++++++++++\n",
      "{'Recall@20': 0.001953125, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.0001953125, 'NDCG@5': tensor(7.5557e-05, device='cuda:0')}\n",
      "one epoch val: 6.252651691436768\n",
      "++++++++++++++++++++ Train Epoch 10 ++++++++++++++++++++\n",
      "Training CE Loss: 4.11521\n",
      "one epoch train: 1.8945374488830566\n",
      "++++++++++++++++++++ Valid Epoch 10 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0030052629765123127, 'NDCG@20': tensor(0.0013, device='cuda:0'), 'Recall@5': 0.0012474504765123129, 'NDCG@5': tensor(0.0008, device='cuda:0')}\n",
      "one epoch val: 6.23137354850769\n",
      "++++++++++++++++++++ Train Epoch 11 ++++++++++++++++++++\n",
      "Training CE Loss: 3.71894\n",
      "one epoch train: 2.115572690963745\n",
      "++++++++++++++++++++ Valid Epoch 11 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0028855259530246257, 'NDCG@20': tensor(0.0009, device='cuda:0'), 'Recall@5': 0.0012474504765123129, 'NDCG@5': tensor(0.0005, device='cuda:0')}\n",
      "one epoch val: 6.22436785697937\n",
      "++++++++++++++++++++ Train Epoch 12 ++++++++++++++++++++\n",
      "Training CE Loss: 3.41622\n",
      "one epoch train: 1.913377285003662\n",
      "++++++++++++++++++++ Valid Epoch 12 ++++++++++++++++++++\n",
      "{'Recall@20': 0.002809950476512313, 'NDCG@20': tensor(0.0010, device='cuda:0'), 'Recall@5': 0.001052137976512313, 'NDCG@5': tensor(0.0005, device='cuda:0')}\n",
      "one epoch val: 6.2725419998168945\n",
      "++++++++++++++++++++ Train Epoch 13 ++++++++++++++++++++\n",
      "Training CE Loss: 3.18150\n",
      "one epoch train: 2.1191606521606445\n",
      "++++++++++++++++++++ Valid Epoch 13 ++++++++++++++++++++\n",
      "{'Recall@20': 0.00315641388297081, 'NDCG@20': tensor(0.0014, device='cuda:0'), 'Recall@5': 0.0008568254765123129, 'NDCG@5': tensor(0.0008, device='cuda:0')}\n",
      "one epoch val: 6.25120735168457\n",
      "++++++++++++++++++++ Train Epoch 14 ++++++++++++++++++++\n",
      "Training CE Loss: 3.00473\n",
      "one epoch train: 1.9047057628631592\n",
      "++++++++++++++++++++ Valid Epoch 14 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0020287004765123127, 'NDCG@20': tensor(0.0008, device='cuda:0'), 'Recall@5': 0.0005859375, 'NDCG@5': tensor(0.0004, device='cuda:0')}\n",
      "one epoch val: 6.3445494174957275\n",
      "++++++++++++++++++++ Train Epoch 15 ++++++++++++++++++++\n",
      "Training CE Loss: 2.87363\n",
      "one epoch train: 1.9304828643798828\n",
      "++++++++++++++++++++ Valid Epoch 15 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0032319894060492517, 'NDCG@20': tensor(0.0017, device='cuda:0'), 'Recall@5': 0.0019089634530246257, 'NDCG@5': tensor(0.0013, device='cuda:0')}\n",
      "one epoch val: 6.393292665481567\n",
      "++++++++++++++++++++ Train Epoch 16 ++++++++++++++++++++\n",
      "Training CE Loss: 2.78605\n",
      "one epoch train: 1.8997597694396973\n",
      "++++++++++++++++++++ Valid Epoch 16 ++++++++++++++++++++\n",
      "{'Recall@20': 0.003080838453024626, 'NDCG@20': tensor(0.0012, device='cuda:0'), 'Recall@5': 0.001052137976512313, 'NDCG@5': tensor(0.0006, device='cuda:0')}\n",
      "one epoch val: 6.318563222885132\n",
      "++++++++++++++++++++ Train Epoch 17 ++++++++++++++++++++\n",
      "Training CE Loss: 2.67355\n",
      "one epoch train: 1.9049019813537598\n",
      "++++++++++++++++++++ Valid Epoch 17 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0013671875, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.000390625, 'NDCG@5': tensor(0.0003, device='cuda:0')}\n",
      "one epoch val: 6.2456042766571045\n",
      "++++++++++++++++++++ Train Epoch 18 ++++++++++++++++++++\n",
      "Training CE Loss: 2.61868\n",
      "one epoch train: 1.9063735008239746\n",
      "++++++++++++++++++++ Valid Epoch 18 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0005859375, 'NDCG@20': tensor(0.0002, device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n",
      "one epoch val: 6.240139722824097\n",
      "++++++++++++++++++++ Train Epoch 19 ++++++++++++++++++++\n",
      "Training CE Loss: 2.54186\n",
      "one epoch train: 1.9049277305603027\n",
      "++++++++++++++++++++ Valid Epoch 19 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0024949009530246258, 'NDCG@20': tensor(0.0009, device='cuda:0'), 'Recall@5': 0.0011277134530246259, 'NDCG@5': tensor(0.0006, device='cuda:0')}\n",
      "one epoch val: 6.235204458236694\n",
      "++++++++++++++++++++ Train Epoch 20 ++++++++++++++++++++\n",
      "Training CE Loss: 2.51220\n",
      "one epoch train: 1.899247407913208\n",
      "++++++++++++++++++++ Valid Epoch 20 ++++++++++++++++++++\n",
      "{'Recall@20': 0.002104275953024626, 'NDCG@20': tensor(0.0010, device='cuda:0'), 'Recall@5': 0.0013230259530246258, 'NDCG@5': tensor(0.0008, device='cuda:0')}\n",
      "one epoch val: 6.31304144859314\n",
      "train_time: 38.92230701446533\n",
      "val_time: 125.15696835517883\n",
      "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
      "{'Recall@20': 0.002809950476512313, 'NDCG@20': tensor(0.0010, device='cuda:0'), 'Recall@5': 0.001052137976512313, 'NDCG@5': tensor(0.0005, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "print('============= Hyperparameter Tuning - cold_start =============')\n",
    "train_loader, val_loader, test_loader = get_data(args) #, user_noclick\n",
    "\n",
    "print('generate sample from SBR file')\n",
    "generate_sbr_sample()\n",
    "\n",
    "# change the learing rate\n",
    "\n",
    "args['lr']  = 0.0003\n",
    "\n",
    "##########################################################\n",
    "\n",
    "\n",
    "process_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711e80d9",
   "metadata": {},
   "source": [
    "## Results\n",
    "### After tuning hyper-parameters\n",
    "\n",
    "### we can summarize the evaluation results as:\n",
    "\n",
    "#### Recall@20 : 0.002809950476512313\n",
    "\n",
    "#### NDCG@20 : 0.0010\n",
    "\n",
    "#### Recall@5 : 0.001052137976512313\n",
    "\n",
    "#### NDCG@5 : 0.0005\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d6e968",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning - Trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f11e84c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Hyperparameter Tuning 2 - cold_start =============\n",
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting user ids and item ids in DataFrame ==========\n",
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting item ids in DataFrame ==========\n",
      "+++user_history+++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15187/15187 [00:00<00:00, 68700.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate sample from SBR file\n",
      "sample size :  user_id           10000000\n",
      "item_id           10000000\n",
      "click             10000000\n",
      "follow            10000000\n",
      "like              10000000\n",
      "share             10000000\n",
      "video_category    10000000\n",
      "watching_times    10000000\n",
      "gender            10000000\n",
      "age               10000000\n",
      "dtype: int64\n",
      "pretrain\n",
      "++++++++++++++++++++ Train Epoch 1 ++++++++++++++++++++\n",
      "Training CE Loss: 10.83504\n",
      "one epoch train: 1.9172673225402832\n",
      "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0021484375, 'NDCG@20': tensor(0.0007, device='cuda:0'), 'Recall@5': 0.0005859375, 'NDCG@5': tensor(0.0003, device='cuda:0')}\n",
      "one epoch val: 6.397033214569092\n",
      "++++++++++++++++++++ Train Epoch 2 ++++++++++++++++++++\n",
      "Training CE Loss: 8.67193\n",
      "one epoch train: 1.9171626567840576\n",
      "++++++++++++++++++++ Valid Epoch 2 ++++++++++++++++++++\n",
      "{'Recall@20': 0.001995400432497263, 'NDCG@20': tensor(0.0007, device='cuda:0'), 'Recall@5': 0.0008235254324972629, 'NDCG@5': tensor(0.0003, device='cuda:0')}\n",
      "one epoch val: 6.311400651931763\n",
      "++++++++++++++++++++ Train Epoch 3 ++++++++++++++++++++\n",
      "Training CE Loss: 7.13552\n",
      "one epoch train: 2.1195473670959473\n",
      "++++++++++++++++++++ Valid Epoch 3 ++++++++++++++++++++\n",
      "{'Recall@20': 0.003837763797491789, 'NDCG@20': tensor(0.0014, device='cuda:0'), 'Recall@5': 0.0014517383649945259, 'NDCG@5': tensor(0.0008, device='cuda:0')}\n",
      "one epoch val: 6.411065340042114\n",
      "++++++++++++++++++++ Train Epoch 4 ++++++++++++++++++++\n",
      "Training CE Loss: 5.84983\n",
      "one epoch train: 1.9018356800079346\n",
      "++++++++++++++++++++ Valid Epoch 4 ++++++++++++++++++++\n",
      "{'Recall@20': 0.003489414229989052, 'NDCG@20': tensor(0.0013, device='cuda:0'), 'Recall@5': 0.000628212932497263, 'NDCG@5': tensor(0.0005, device='cuda:0')}\n",
      "one epoch val: 6.223386526107788\n",
      "++++++++++++++++++++ Train Epoch 5 ++++++++++++++++++++\n",
      "Training CE Loss: 4.88157\n",
      "one epoch train: 1.9086809158325195\n",
      "++++++++++++++++++++ Valid Epoch 5 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0015625, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.0005859375, 'NDCG@5': tensor(0.0003, device='cuda:0')}\n",
      "one epoch val: 6.263377904891968\n",
      "++++++++++++++++++++ Train Epoch 6 ++++++++++++++++++++\n",
      "Training CE Loss: 4.14727\n",
      "one epoch train: 2.09957218170166\n",
      "++++++++++++++++++++ Valid Epoch 6 ++++++++++++++++++++\n",
      "{'Recall@20': 0.002428300864994526, 'NDCG@20': tensor(0.0009, device='cuda:0'), 'Recall@5': 0.0005859375, 'NDCG@5': tensor(0.0004, device='cuda:0')}\n",
      "one epoch val: 6.273785591125488\n",
      "++++++++++++++++++++ Train Epoch 7 ++++++++++++++++++++\n",
      "Training CE Loss: 3.69865\n",
      "one epoch train: 1.9271438121795654\n",
      "++++++++++++++++++++ Valid Epoch 7 ++++++++++++++++++++\n",
      "{'Recall@20': 0.001409462932497263, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n",
      "one epoch val: 6.38626503944397\n",
      "++++++++++++++++++++ Train Epoch 8 ++++++++++++++++++++\n",
      "Training CE Loss: 3.41541\n",
      "one epoch train: 1.921266794204712\n",
      "++++++++++++++++++++ Valid Epoch 8 ++++++++++++++++++++\n",
      "{'Recall@20': 0.003489414229989052, 'NDCG@20': tensor(0.0016, device='cuda:0'), 'Recall@5': 0.0008235254324972629, 'NDCG@5': tensor(0.0008, device='cuda:0')}\n",
      "one epoch val: 6.373831272125244\n",
      "++++++++++++++++++++ Train Epoch 9 ++++++++++++++++++++\n",
      "Training CE Loss: 3.22707\n",
      "one epoch train: 2.129364490509033\n",
      "++++++++++++++++++++ Valid Epoch 9 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0020376758649945257, 'NDCG@20': tensor(0.0007, device='cuda:0'), 'Recall@5': 0.0005859375, 'NDCG@5': tensor(0.0003, device='cuda:0')}\n",
      "one epoch val: 6.46622896194458\n",
      "++++++++++++++++++++ Train Epoch 10 ++++++++++++++++++++\n",
      "Training CE Loss: 3.10181\n",
      "one epoch train: 1.9197595119476318\n",
      "++++++++++++++++++++ Valid Epoch 10 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0030142383649945257, 'NDCG@20': tensor(0.0015, device='cuda:0'), 'Recall@5': 0.0010188379324972629, 'NDCG@5': tensor(0.0009, device='cuda:0')}\n",
      "one epoch val: 6.32180118560791\n",
      "++++++++++++++++++++ Train Epoch 11 ++++++++++++++++++++\n",
      "Training CE Loss: 2.97866\n",
      "one epoch train: 1.909067153930664\n",
      "++++++++++++++++++++ Valid Epoch 11 ++++++++++++++++++++\n",
      "{'Recall@20': 0.002232988364994526, 'NDCG@20': tensor(0.0006, device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n",
      "one epoch val: 6.361148118972778\n",
      "++++++++++++++++++++ Train Epoch 12 ++++++++++++++++++++\n",
      "Training CE Loss: 2.90135\n",
      "one epoch train: 2.0899434089660645\n",
      "++++++++++++++++++++ Valid Epoch 12 ++++++++++++++++++++\n",
      "{'Recall@20': 0.001214150432497263, 'NDCG@20': tensor(0.0007, device='cuda:0'), 'Recall@5': 0.000628212932497263, 'NDCG@5': tensor(0.0006, device='cuda:0')}\n",
      "one epoch val: 6.3066980838775635\n",
      "++++++++++++++++++++ Train Epoch 13 ++++++++++++++++++++\n",
      "Training CE Loss: 2.84778\n",
      "one epoch train: 1.910954236984253\n",
      "++++++++++++++++++++ Valid Epoch 13 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0036424512974917887, 'NDCG@20': tensor(0.0012, device='cuda:0'), 'Recall@5': 0.0005859375, 'NDCG@5': tensor(0.0004, device='cuda:0')}\n",
      "one epoch val: 6.309591293334961\n",
      "++++++++++++++++++++ Train Epoch 14 ++++++++++++++++++++\n",
      "Training CE Loss: 2.78909\n",
      "one epoch train: 1.9100048542022705\n",
      "++++++++++++++++++++ Valid Epoch 14 ++++++++++++++++++++\n",
      "{'Recall@20': 0.001800087932497263, 'NDCG@20': tensor(0.0007, device='cuda:0'), 'Recall@5': 0.000390625, 'NDCG@5': tensor(0.0003, device='cuda:0')}\n",
      "one epoch val: 6.354692459106445\n",
      "++++++++++++++++++++ Train Epoch 15 ++++++++++++++++++++\n",
      "Training CE Loss: 2.74190\n",
      "one epoch train: 1.9090821743011475\n",
      "++++++++++++++++++++ Valid Epoch 15 ++++++++++++++++++++\n",
      "{'Recall@20': 0.003098789229989052, 'NDCG@20': tensor(0.0009, device='cuda:0'), 'Recall@5': 0.000628212932497263, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 6.355791091918945\n",
      "++++++++++++++++++++ Train Epoch 16 ++++++++++++++++++++\n",
      "Training CE Loss: 2.68780\n",
      "one epoch train: 2.127936601638794\n",
      "++++++++++++++++++++ Valid Epoch 16 ++++++++++++++++++++\n",
      "{'Recall@20': 0.002512851729989052, 'NDCG@20': tensor(0.0010, device='cuda:0'), 'Recall@5': 0.000628212932497263, 'NDCG@5': tensor(0.0005, device='cuda:0')}\n",
      "one epoch val: 6.271270513534546\n",
      "++++++++++++++++++++ Train Epoch 17 ++++++++++++++++++++\n",
      "Training CE Loss: 2.64274\n",
      "one epoch train: 1.9174563884735107\n",
      "++++++++++++++++++++ Valid Epoch 17 ++++++++++++++++++++\n",
      "{'Recall@20': 0.003209550864994526, 'NDCG@20': tensor(0.0012, device='cuda:0'), 'Recall@5': 0.00078125, 'NDCG@5': tensor(0.0005, device='cuda:0')}\n",
      "one epoch val: 6.313340663909912\n",
      "++++++++++++++++++++ Train Epoch 18 ++++++++++++++++++++\n",
      "Training CE Loss: 2.62731\n",
      "one epoch train: 1.9358525276184082\n",
      "++++++++++++++++++++ Valid Epoch 18 ++++++++++++++++++++\n",
      "{'Recall@20': 0.002232988364994526, 'NDCG@20': tensor(0.0008, device='cuda:0'), 'Recall@5': 0.00043290043249726294, 'NDCG@5': tensor(0.0003, device='cuda:0')}\n",
      "one epoch val: 6.315901756286621\n",
      "++++++++++++++++++++ Train Epoch 19 ++++++++++++++++++++\n",
      "Training CE Loss: 2.62158\n",
      "one epoch train: 1.9140372276306152\n",
      "++++++++++++++++++++ Valid Epoch 19 ++++++++++++++++++++\n",
      "{'Recall@20': 0.001604775432497263, 'NDCG@20': tensor(0.0010, device='cuda:0'), 'Recall@5': 0.0010188379324972629, 'NDCG@5': tensor(0.0008, device='cuda:0')}\n",
      "one epoch val: 6.335180282592773\n",
      "++++++++++++++++++++ Train Epoch 20 ++++++++++++++++++++\n",
      "Training CE Loss: 2.60615\n",
      "one epoch train: 2.1033623218536377\n",
      "++++++++++++++++++++ Valid Epoch 20 ++++++++++++++++++++\n",
      "{'Recall@20': 0.003209550864994526, 'NDCG@20': tensor(0.0013, device='cuda:0'), 'Recall@5': 0.0010188379324972629, 'NDCG@5': tensor(0.0007, device='cuda:0')}\n",
      "one epoch val: 6.360570430755615\n",
      "train_time: 39.48929762840271\n",
      "val_time: 126.71236038208008\n",
      "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0012132154311984777, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "print('============= Hyperparameter Tuning 2 - cold_start =============')\n",
    "train_loader, val_loader, test_loader = get_data(args) #, user_noclick\n",
    "\n",
    "print('generate sample from SBR file')\n",
    "generate_sbr_sample()\n",
    "\n",
    "# change learning rate and patch size\n",
    "args['train_batch_size'] = 2048\n",
    "\n",
    "args['val_batch_size'] = 2048\n",
    "\n",
    "args['test_batch_size'] = 2048\n",
    "\n",
    "args['lr']  = 0.0008\n",
    "\n",
    "##########################################################\n",
    "process_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea3c82",
   "metadata": {},
   "source": [
    "## Results\n",
    "### After tuning hyper-parameters\n",
    "\n",
    "### we can summarize the evaluation results as:\n",
    "\n",
    "\n",
    "#### Recall@20 : 0.0012132154311984777\n",
    "\n",
    "#### NDCG@20 : 0.0003\n",
    "\n",
    "#### Recall@5 : 0.0\n",
    "\n",
    "#### NDCG@5 : 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e0f6b",
   "metadata": {},
   "source": [
    "# After tuning hyper-parameters we can say that the performance became better (trial 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ee532",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://github.com/yuangh-x/2022-NIPS-Tenrec\n",
    "\n",
    "https://tenrec0.github.io/assets/doc/NIPS2022-Tenrec.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
