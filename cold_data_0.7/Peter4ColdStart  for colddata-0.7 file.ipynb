{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b67a83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T07:53:05.593038Z",
     "iopub.status.busy": "2023-07-25T07:53:05.592437Z",
     "iopub.status.idle": "2023-07-25T07:53:09.942630Z",
     "shell.execute_reply": "2023-07-25T07:53:09.941440Z",
     "shell.execute_reply.started": "2023-07-25T07:53:05.593001Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import joblib\n",
    "import pickle\n",
    "import torch.utils.data as data_utils\n",
    "import scipy.sparse as sp\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "969c5ec7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T07:53:09.945384Z",
     "iopub.status.busy": "2023-07-25T07:53:09.944780Z",
     "iopub.status.idle": "2023-07-25T07:53:17.706215Z",
     "shell.execute_reply": "2023-07-25T07:53:17.705178Z",
     "shell.execute_reply.started": "2023-07-25T07:53:09.945347Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Set to the index of the GPU you want to use\n",
    "# torch.cuda.set_per_process_memory_fraction(0.8)  # Limit memory usage to 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34252e35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T07:53:17.708673Z",
     "iopub.status.busy": "2023-07-25T07:53:17.707903Z",
     "iopub.status.idle": "2023-07-25T07:53:17.743621Z",
     "shell.execute_reply": "2023-07-25T07:53:17.742222Z",
     "shell.execute_reply.started": "2023-07-25T07:53:17.708637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c62dd",
   "metadata": {},
   "source": [
    "# Functions To Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e21c7d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T07:53:17.748028Z",
     "iopub.status.busy": "2023-07-25T07:53:17.747283Z",
     "iopub.status.idle": "2023-07-25T07:53:17.913398Z",
     "shell.execute_reply": "2023-07-25T07:53:17.912374Z",
     "shell.execute_reply.started": "2023-07-25T07:53:17.747994Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_sampler(train_data, val_data, test_data, user_count, item_count, args):\n",
    "    if args.sample == 'random':\n",
    "        return RandomNegativeSampler(train_data, val_data, test_data, user_count, item_count, args.negsample_size, args.seed, args.negsample_savefolder)\n",
    "    elif args.sample == 'popular':\n",
    "        return PopularNegativeSampler(train_data, val_data, test_data, user_count, item_count, args.negsample_size, args.seed, args.negsample_savefolder)\n",
    "\n",
    "def mtl_data(path=None, args=None):\n",
    "    if not path:\n",
    "        return\n",
    "    df = pd.read_csv(path, usecols=[\"user_id\", \"item_id\", \"click\", \"like\", \"video_category\", \"gender\", \"age\", \"hist_1\", \"hist_2\",\n",
    "                       \"hist_3\", \"hist_4\", \"hist_5\", \"hist_6\", \"hist_7\", \"hist_8\", \"hist_9\", \"hist_10\"])\n",
    "   \n",
    "    df['video_category'] = df['video_category'].astype(str)\n",
    "    df = sample_data(df)\n",
    "    if args.mtl_task_num == 2:\n",
    "        label_columns = ['click', 'like']\n",
    "        categorical_columns = [\"user_id\", \"item_id\", \"video_category\", \"gender\", \"age\", \"hist_1\", \"hist_2\",\n",
    "                       \"hist_3\", \"hist_4\", \"hist_5\", \"hist_6\", \"hist_7\", \"hist_8\", \"hist_9\", \"hist_10\"]\n",
    "    elif args.mtl_task_num == 1:\n",
    "        label_columns = ['click']\n",
    "        categorical_columns = [\"user_id\", \"item_id\", \"video_category\", \"gender\", \"age\", \"hist_1\", \"hist_2\",\n",
    "                               \"hist_3\", \"hist_4\", \"hist_5\", \"hist_6\", \"hist_7\", \"hist_8\", \"hist_9\", \"hist_10\"]\n",
    "    else:\n",
    "        label_columns = ['like']\n",
    "        categorical_columns = [\"user_id\", \"item_id\", \"video_category\", \"gender\", \"age\", \"hist_1\", \"hist_2\",\n",
    "                               \"hist_3\", \"hist_4\", \"hist_5\", \"hist_6\", \"hist_7\", \"hist_8\", \"hist_9\", \"hist_10\"]\n",
    "    user_columns = [\"user_id\", \"gender\", \"age\"]\n",
    "    for col in tqdm(categorical_columns):\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "\n",
    "    new_columns = categorical_columns + label_columns\n",
    "    df = df.reindex(columns=new_columns)\n",
    "\n",
    "    user_feature_dict, item_feature_dict = {}, {}\n",
    "    for idx, col in tqdm(enumerate(df.columns)):\n",
    "        if col not in label_columns:\n",
    "            if col in user_columns:\n",
    "                user_feature_dict[col] = (len(df[col].unique()), idx)\n",
    "            else:\n",
    "                item_feature_dict[col] = (len(df[col].unique()), idx)\n",
    "\n",
    "    df = df.sample(frac=1)\n",
    "    train_len = int(len(df) * 0.8)\n",
    "    train_df = df[:train_len]\n",
    "    tmp_df = df[train_len:]\n",
    "    val_df = tmp_df[:int(len(tmp_df)/2)]\n",
    "    test_df = tmp_df[int(len(tmp_df)/2):]\n",
    "    return train_df, val_df, test_df, user_feature_dict, item_feature_dict\n",
    "\n",
    "def set_fenbu(row_data):\n",
    "    tmp1 = row_data[row_data.click.isin([0])]\n",
    "    tmp2 = row_data[row_data.click.isin([1])]\n",
    "    data = []\n",
    "    j = 0\n",
    "    for i in tqdm(range(int(len(tmp2)/1000))):\n",
    "        data.append(tmp2.iloc[i, :].values.tolist())\n",
    "        data.extend(tmp1.iloc[j : j + 3, :].values.tolist())\n",
    "        j = j + 3\n",
    "    new_data = pd.DataFrame(data, columns=row_data.columns)\n",
    "    return new_data\n",
    "\n",
    "def sample_data(df):\n",
    "    p_df = df[df.click.isin([1])]\n",
    "    n_df = df[df.click.isin([0])]\n",
    "    del df\n",
    "    n_df = n_df.sample(n=len(p_df)*2)\n",
    "    df = p_df.append(n_df)\n",
    "    del p_df, n_df\n",
    "    df = df.sample(frac=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def gen_list(hist):\n",
    "    data = []\n",
    "    for key, value in tqdm(hist.items()):\n",
    "        for v in value:\n",
    "            data.append([key, v])\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_ur(df):\n",
    "    \"\"\"\n",
    "    Method of getting user-rating pairs\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame, rating dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ur : dict, dictionary stored user-items interactions\n",
    "    \"\"\"\n",
    "    print(\"Method of getting user-rating pairs\")\n",
    "    ur = df.groupby('user_id').item_id.apply(list).to_dict()\n",
    "    return ur\n",
    "\n",
    "def category_encoding(df):\n",
    "    df['user_id'] = pd.Categorical(df['user_id']).codes\n",
    "    df['item_id'] = pd.Categorical(df['item_id']).codes\n",
    "    return df\n",
    "\n",
    "def gen_hist_matrix(data, user_num, item_num, train_dict):\n",
    "    max_len = 0\n",
    "    for _, v in train_dict.items():\n",
    "        if max_len < len(v):\n",
    "            max_len = len(v)\n",
    "    if max_len > item_num * 0.2:\n",
    "        print(f'Max value of user history interaction records has reached: {max_len / item_num * 100:.4f}% of the total.')\n",
    "    history_matrix = np.zeros((user_num+1, max_len), dtype=np.int64)\n",
    "    history_value = np.zeros((user_num+1, max_len))\n",
    "    history_len = np.zeros(user_num+1, dtype=np.int64)\n",
    "\n",
    "    for user, item in data:\n",
    "        history_matrix[user, history_len[user]] = item\n",
    "        history_value[user, history_len[user]] = 1\n",
    "        history_len[user] += 1\n",
    "    return torch.LongTensor(history_matrix), torch.FloatTensor(history_value), torch.LongTensor(history_len)\n",
    "\n",
    "\n",
    "def get_history_matrix(df, args, row='user_id', use_config_value_name=False):\n",
    "    '''\n",
    "    get the history interactions by user/item\n",
    "    '''\n",
    "    # logger = config['logger']\n",
    "    assert row in df.columns, f'invalid name {row}: not in columns of history dataframe'\n",
    "    # uid_name, iid_name  = config['UID_NAME'], config['IID_NAME']\n",
    "    user_ids, item_ids = df['user_id'].values, df['item_id'].values\n",
    "    value_name = 'click' if use_config_value_name else None\n",
    "\n",
    "    user_num, item_num = args.num_users, args.num_items\n",
    "    values = np.ones(len(df)) if value_name is None else df[value_name].values\n",
    "\n",
    "    if row == 'user':\n",
    "        row_num, max_col_num = user_num, item_num\n",
    "        row_ids, col_ids = user_ids, item_ids\n",
    "    else: # 'item'\n",
    "        row_num, max_col_num = item_num, user_num\n",
    "        row_ids, col_ids = item_ids, user_ids\n",
    "\n",
    "    history_len = np.zeros(row_num, dtype=np.int16)\n",
    "    for row_id in row_ids:\n",
    "        history_len[row_id] += 1\n",
    "\n",
    "    col_num = np.max(history_len)\n",
    "    col_num = col_num.astype(int)\n",
    "    if col_num > max_col_num * 0.2:\n",
    "        print(f'Max value of {row}\\'s history interaction records has reached: {col_num / max_col_num * 100:.4f}% of the total.')\n",
    "\n",
    "    history_matrix = np.zeros((row_num, col_num), dtype=np.int16)\n",
    "    history_value = np.zeros((row_num, col_num), dtype=np.int16)\n",
    "    history_len[:] = 0\n",
    "    for row_id, value, col_id in zip(row_ids, values, col_ids):\n",
    "        history_matrix[row_id, history_len[row_id]] = col_id\n",
    "        history_value[row_id, history_len[row_id]] = value\n",
    "        history_len[row_id] += 1\n",
    "\n",
    "    return torch.LongTensor(history_matrix), torch.FloatTensor(history_value), torch.LongTensor(history_len)\n",
    "\n",
    "def get_inter_matrix(df, args, form='coo'):\n",
    "    '''\n",
    "    get the whole sparse interaction matrix\n",
    "    '''\n",
    "    print(\"get the whole sparse interaction matrix\")\n",
    "    user_num, item_num = args.num_users, args.num_items\n",
    "\n",
    "    src, tar = df['user_id'].values, df['item_id'].values\n",
    "    data = df['click'].values\n",
    "\n",
    "    mat = sp.coo_matrix((data, (src, tar)), shape=(user_num, item_num))\n",
    "\n",
    "    if form == 'coo':\n",
    "        return mat\n",
    "    elif form == 'csr':\n",
    "        return mat.tocsr()\n",
    "    else:\n",
    "        raise NotImplementedError(f'Sparse matrix format [{form}] has not been implemented...')\n",
    "\n",
    "\n",
    "def build_candidates_set(test_ur, train_ur, args, drop_past_inter=True):\n",
    "    \"\"\"\n",
    "    method of building candidate items for ranking\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_ur : dict, ground_truth that represents the relationship of user and item in the test set\n",
    "    train_ur : dict, this represents the relationship of user and item in the train set\n",
    "    item_num : No. of all items\n",
    "    cand_num : int, the number of candidates\n",
    "    drop_past_inter : drop items already appeared in train set\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    test_ucands : dict, dictionary storing candidates for each user in test set\n",
    "    \"\"\"\n",
    "    item_num = args.num_items\n",
    "    candidates_num = args.cand_num\n",
    "\n",
    "    test_ucands, test_u = [], []\n",
    "    for u, r in tqdm(test_ur.items()):\n",
    "        sample_num = candidates_num - len(r) if len(r) <= candidates_num else 0\n",
    "        if sample_num == 0:\n",
    "            samples = np.random.choice(list(r), candidates_num)\n",
    "        else:\n",
    "            pos_items = list(r) + list(train_ur[u]) if drop_past_inter else list(r)\n",
    "            # neg_items = np.setdiff1d(np.arange(item_num), pos_items)\n",
    "            # samples = np.random.choice(neg_items, size=sample_num)\n",
    "            samples = []\n",
    "            for _ in range(sample_num):\n",
    "                item = np.random.choice(item_num)\n",
    "                while item in pos_items or item in samples:\n",
    "                    item = np.random.choice(item_num)\n",
    "                samples.append(item)\n",
    "            samples = np.array(samples)\n",
    "            samples = np.concatenate((samples, list(r)), axis=None)\n",
    "\n",
    "        test_ucands.append([u, samples])\n",
    "        test_u.append(u)\n",
    "\n",
    "    return test_u, test_ucands\n",
    "\n",
    "class BasicDataset(data_utils.Dataset):\n",
    "    def __init__(self, samples):\n",
    "        '''\n",
    "        convert array-like <u, i, j> / <u, i, r> / <target_i, context_i, label>\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples : np.array\n",
    "            samples generated by sampler\n",
    "        '''\n",
    "        super(BasicDataset, self).__init__()\n",
    "        self.data = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index][0], self.data[index][1], self.data[index][2]\n",
    "\n",
    "\n",
    "def split_warm_hot(df, item_min):\n",
    "    user_counts = df.groupby('user_id').size()\n",
    "    w_user_subset = np.in1d(df.user_id, user_counts[user_counts >= item_min].index)\n",
    "    c_user_subset = np.in1d(df.user_id, user_counts[(user_counts <= 5) & (user_counts > 1)].index)\n",
    "    w_filter_df = df[w_user_subset].reset_index(drop=True)\n",
    "    c_filter_df = df[c_user_subset].reset_index(drop=True)\n",
    "    return w_filter_df, c_filter_df\n",
    "\n",
    "def transferdataset(args):\n",
    "    path = args.path\n",
    "    item_min = args.item_min\n",
    "    if not path:\n",
    "        return\n",
    "    df = pd.read_csv(path, usecols=['user_id', 'item_id', 'click'])\n",
    "\n",
    "    df = df[df.click.isin([1])]\n",
    "    user_counts = df.groupby('user_id').size()\n",
    "    user_subset = np.in1d(df.user_id, user_counts[user_counts >= item_min].index)\n",
    "    filter_df = df[user_subset].reset_index(drop=True)\n",
    "\n",
    "    user_count = len(set(filter_df['user_id']))\n",
    "    item_count = len(set(filter_df['item_id']))\n",
    "\n",
    "    assert (filter_df.groupby('user_id').size() < item_min).sum() == 0\n",
    "    del df\n",
    "\n",
    "    reset_ob = reset_df()\n",
    "    filter_df = reset_ob.fit_transform(filter_df)\n",
    "\n",
    "    user_history = {}\n",
    "    print(\"+++user_history+++\")\n",
    "    savefile_path = Path(\"data/history_{}.pkl\".format(args.seed))\n",
    "    if savefile_path.is_file():\n",
    "        with open(savefile_path, \"rb\") as load_f:\n",
    "            user_history = pickle.load(load_f)\n",
    "    else:\n",
    "        for uid in tqdm(filter_df.user_id.unique()):\n",
    "            dataframe = filter_df[filter_df.user_id == uid].item_id\n",
    "            sequence = dataframe.values.tolist()\n",
    "            user_history[uid] = sequence\n",
    "        with open(savefile_path, \"wb\") as dump_f:\n",
    "            pickle.dump(user_history, dump_f)\n",
    "    return filter_df, user_history, user_count, item_count\n",
    "\n",
    "def sequencedataset(item_min, args, path=None):\n",
    "    if '2' in path:\n",
    "        df = pd.read_csv(path, usecols=['user_id', 'item_id', 'like'])\n",
    "        df = df[df.like.isin([1])]\n",
    "    else:\n",
    "        df = pd.read_csv(path, usecols=['user_id', 'item_id', 'click'])\n",
    "        df = df[df.click.isin([1])]\n",
    "    user_counts = df.groupby('user_id').size()\n",
    "    user_subset = np.in1d(df.user_id, user_counts[user_counts >= item_min].index)\n",
    "    filter_df = df[user_subset].reset_index(drop=True)\n",
    "\n",
    "    assert (filter_df.groupby('user_id').size() < item_min).sum() == 0\n",
    "    user_count = len(set(filter_df['user_id']))\n",
    "    item_count = len(set(filter_df['item_id']))\n",
    "    del df\n",
    "\n",
    "    reset_ob = reset_df()\n",
    "    filter_df = reset_ob.fit_transform(filter_df)\n",
    "    print(\"+++user_history+++\")\n",
    "    user_history = filter_df.groupby('user_id').item_id.apply(list).to_dict()\n",
    "    return filter_df, user_history, user_count, item_count\n",
    "\n",
    "def data_count(df, item_min, target=False):\n",
    "    user_counts = df.groupby('user_id').size()\n",
    "    if target:\n",
    "        filter_df = df\n",
    "    else:\n",
    "        user_subset = np.in1d(df.user_id, user_counts[user_counts >= item_min].index)\n",
    "        filter_df = df[user_subset].reset_index(drop=True)\n",
    "\n",
    "        assert (filter_df.groupby('user_id').size() < item_min).sum() == 0\n",
    "    user_count = len(set(filter_df['user_id']))\n",
    "    item_count = len(set(filter_df['item_id']))\n",
    "    return filter_df, user_count, item_count\n",
    "\n",
    "\n",
    "def construct_data(args, item_min):\n",
    "    path1 = args['target_path']\n",
    "    path2 = args['source_path']\n",
    "    df1 = pd.read_csv(path1, usecols=['user_id', 'item_id', 'click'])\n",
    "    df1 = df1[df1.click.isin([1])]\n",
    "    df2 = pd.read_csv(path2, usecols=['user_id', 'item_id', 'click'])\n",
    "    df2 = df2[df2.click.isin([1])]\n",
    "    user_counts = df2.groupby('user_id').size()\n",
    "    user_subset = np.in1d(df2.user_id, user_counts[user_counts >= item_min].index)\n",
    "    df2 = df2[user_subset].reset_index(drop=True)\n",
    "\n",
    "    assert (df2.groupby('user_id').size() < item_min).sum() == 0\n",
    "    s_item_count = len(set(df2['item_id']))\n",
    "    reset_ob = cold_reset_df()\n",
    "    df2, df1 = reset_ob.fit_transform(df2, df1)\n",
    "    user1 = set(df1.user_id.values.tolist())\n",
    "    user2 = set(df2.user_id.values.tolist())\n",
    "    user = user1 & user2\n",
    "    df1 = df1[df1.user_id.isin(list(user))]\n",
    "    df2 = df2[df2.user_id.isin(list(user))]\n",
    "    new_data1 = []\n",
    "    new_data2 = []\n",
    "    for u in user:\n",
    "        tmp_data2 = df2[df2.user_id == u][:-3].values.tolist()\n",
    "        tmp_data1 = df1[df1.user_id == u].values.tolist()\n",
    "        new_data1.extend(tmp_data1)\n",
    "        new_data2.extend(tmp_data2)\n",
    "        \n",
    "    new_data1 = pd.DataFrame(new_data1, columns=df1.columns)\n",
    "    new_data2 = pd.DataFrame(new_data2, columns=df2.columns)\n",
    "    user_count = len(set(new_data1.user_id.values.tolist()))\n",
    "    reset_item = item_reset_df()\n",
    "    new_data1 = reset_item.fit_transform(new_data1)\n",
    "    t_item_count = len(set(new_data1['item_id']))\n",
    "    return new_data1, new_data2, user_count, t_item_count, s_item_count\n",
    "\n",
    "\n",
    "def construct_ch_data(args, item_min):\n",
    "    path1 = args['target_path']\n",
    "    path2 = args['source_path']\n",
    "\n",
    "    df1 = pd.read_csv(path1, usecols=['user_id', 'item_id', 'click'])\n",
    "    df1 = df1[df1.click.isin([1])]\n",
    "    df2 = pd.read_csv(path2, usecols=['user_id', 'item_id', 'click'])\n",
    "    df2 = df2[df2.click.isin([1])]\n",
    "\n",
    "    user_counts = df2.groupby('user_id').size()\n",
    "    user_subset = np.in1d(df2.user_id, user_counts[user_counts >= item_min].index)\n",
    "    df2 = df2[user_subset].reset_index(drop=True)\n",
    "\n",
    "    assert (df2.groupby('user_id').size() < item_min).sum() == 0\n",
    "    s_item_count = len(set(df2['item_id']))\n",
    "    reset_ob = cold_reset_df()\n",
    "    df2, df1 = reset_ob.fit_transform(df2, df1)\n",
    "\n",
    "    user1 = set(df1.user_id.values.tolist())\n",
    "    user2 = set(df2.user_id.values.tolist())\n",
    "    user = user1 & user2\n",
    "    df1 = df1[df1.user_id.isin(list(user))]\n",
    "    df2 = df2[df2.user_id.isin(list(user))]\n",
    "\n",
    "    # cold and hot user\n",
    "    user_counts1 = df1.groupby('user_id').size()\n",
    "    cold_user_ind = np.in1d(df1.user_id, user_counts1[user_counts1 <= 5].index)\n",
    "    hot_user_ind = np.in1d(df1.user_id, user_counts1[user_counts1 > 5].index)\n",
    "\n",
    "    cold_user = set(df1[cold_user_ind].user_id.values.tolist())\n",
    "    hot_user = set(df1[hot_user_ind].user_id.values.tolist())\n",
    "\n",
    "    new_data1 = []\n",
    "    new_data2 = []\n",
    "    for u in user:\n",
    "        tmp_data2 = df2[df2.user_id == u][:-3].values.tolist()\n",
    "        tmp_data1 = df1[df1.user_id == u].values.tolist()\n",
    "        new_data1.extend(tmp_data1)\n",
    "        new_data2.extend(tmp_data2)\n",
    "    new_data1 = pd.DataFrame(new_data1, columns=df1.columns)\n",
    "    new_data2 = pd.DataFrame(new_data2, columns=df2.columns)\n",
    "    user_count = len(set(new_data1.user_id.values.tolist()))\n",
    "    reset_item = item_reset_df()\n",
    "    new_data1 = reset_item.fit_transform(new_data1)\n",
    "    t_item_count = len(set(new_data1['item_id']))\n",
    "    return new_data1, new_data2, user_count, t_item_count, s_item_count, cold_user, hot_user\n",
    "\n",
    "def colddataset(item_min, args, path=None):\n",
    "    target_data, source_data, user_count, t_item_count, s_item_count = construct_data(args, item_min)\n",
    "    print(\"+++user_history+++\")\n",
    "    user_history = source_data.groupby('user_id').item_id.apply(list).to_dict()\n",
    "    target = target_data.groupby('user_id').item_id.apply(list).to_dict()\n",
    "\n",
    "    examples = []\n",
    "    for u, t_list in tqdm(target.items()):\n",
    "        for t in t_list:\n",
    "            e_list = [user_history[u] + [0], t]\n",
    "            examples.append(e_list)\n",
    "    examples = pd.DataFrame(examples, columns=['source', 'target'])\n",
    "    return examples, user_count, s_item_count, t_item_count\n",
    "\n",
    "\n",
    "def utils(df, args):\n",
    "    if args.user_profile == 'gender':\n",
    "        df = df[df.gender != 0]\n",
    "        df = df[df.click.isin([1])]\n",
    "        df['gender'] = df['gender'] - 1\n",
    "        user_counts = df.groupby('user_id').size()\n",
    "        user_subset = np.in1d(df.user_id, user_counts[user_counts >= args.item_min].index)\n",
    "        filter_df = df[user_subset].reset_index(drop=True)\n",
    "    elif args.user_profile == 'age':\n",
    "        df = df[df.age != 0]\n",
    "        df = df[df.click.isin([1])]\n",
    "        df['age'] = df['age'] - 1\n",
    "        user_counts = df.groupby('user_id').size()\n",
    "        user_subset = np.in1d(df.user_id, user_counts[user_counts >= args.item_min].index)\n",
    "        filter_df = df[user_subset].reset_index(drop=True)\n",
    "\n",
    "    return filter_df\n",
    "\n",
    "def gender_df(filter_df, args):\n",
    "    if args.user_profile == 'gender':\n",
    "        gender_list = []\n",
    "        for uid in tqdm(filter_df.user_id.unique()):\n",
    "            dataframe = filter_df[filter_df.user_id == uid].item_id\n",
    "            sequence = dataframe.values.tolist()\n",
    "            gender = filter_df[filter_df.user_id == uid].gender.values[0]\n",
    "            list = [uid, sequence, gender]\n",
    "            gender_list.append(list)\n",
    "        profile_df = pd.DataFrame(gender_list, columns=['uid', 'history', 'profile'])\n",
    "    else:\n",
    "        age_list = []\n",
    "        for uid in tqdm(filter_df.user_id.unique()):\n",
    "            dataframe = filter_df[filter_df.user_id == uid].item_id\n",
    "            sequence = dataframe.values.tolist()\n",
    "            age = filter_df[filter_df.user_id == uid].age.values[0]\n",
    "            list = [uid, sequence, age]\n",
    "            age_list.append(list)\n",
    "        profile_df = pd.DataFrame(age_list, columns=['uid', 'history', 'profile'])\n",
    "\n",
    "    return profile_df\n",
    "\n",
    "def train_val_test_split(user_history):\n",
    "    if not user_history:\n",
    "        return\n",
    "    train_history = {}\n",
    "    val_history = {}\n",
    "    test_history = {}\n",
    "    for key, history in tqdm(user_history.items()):\n",
    "        train_history[key] = history[:-2]\n",
    "        val_history[key] = history[-2:-1]\n",
    "        test_history[key] = history[-1:]\n",
    "    return train_history, val_history, test_history\n",
    "\n",
    "def val_test_split(user_history):\n",
    "    if not user_history:\n",
    "        return\n",
    "    val_history = {}\n",
    "    test_history = {}\n",
    "    val_len = int(len(user_history) / 5)\n",
    "    test_len = int(len(user_history))\n",
    "    i = 0\n",
    "    for key, history in tqdm(user_history.items()):\n",
    "        if i < val_len:\n",
    "            val_history[key] = history\n",
    "            i += 1\n",
    "        elif i >= val_len and i < test_len:\n",
    "            test_history[key] = history\n",
    "            i += 1\n",
    "    return val_history, test_history\n",
    "\n",
    "class item_reset_df(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"=\" * 10, \"Initialize Reset DataFrame Object\", \"=\" * 10)\n",
    "        self.item_enc = LabelEncoder()\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "        print(\"=\" * 10, \"Resetting item ids in DataFrame\", \"=\" * 10)\n",
    "        df['item_id'] = self.item_enc.fit_transform(df['item_id']) + 1\n",
    "        return df\n",
    "\n",
    "    def inverse_transform(self, df):\n",
    "        df['item_id'] = self.item_enc.inverse_transform(df['item_id']) - 1\n",
    "        return df\n",
    "\n",
    "class reset_df(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"=\" * 10, \"Initialize Reset DataFrame Object\", \"=\" * 10)\n",
    "        self.item_enc = LabelEncoder()\n",
    "        self.user_enc = LabelEncoder()\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "        print(\"=\" * 10, \"Resetting user ids and item ids in DataFrame\", \"=\" * 10)\n",
    "        df['item_id'] = self.item_enc.fit_transform(df['item_id']) + 1\n",
    "        df['user_id'] = self.user_enc.fit_transform(df['user_id']) + 1\n",
    "        return df\n",
    "\n",
    "    def inverse_transform(self, df):\n",
    "        df['item_id'] = self.item_enc.inverse_transform(df['item_id']) - 1\n",
    "        df['user_id'] = self.user_enc.inverse_transform(df['user_id']) - 1\n",
    "        return df\n",
    "\n",
    "class cold_reset_df(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"=\" * 10, \"Initialize Reset DataFrame Object\", \"=\" * 10)\n",
    "        self.item_enc1 = LabelEncoder()\n",
    "        self.item_enc2 = LabelEncoder()\n",
    "        self.user_enc = LabelEncoder()\n",
    "\n",
    "    def fit_transform(self, df1, df2):\n",
    "        print(\"=\" * 10, \"Resetting user ids and item ids in DataFrame\", \"=\" * 10)\n",
    "        df = pd.concat([df1['user_id'], df2['user_id']])\n",
    "        df = self.user_enc.fit_transform(df) + 1\n",
    "        df1['item_id'] = self.item_enc1.fit_transform(df1['item_id']) + 1\n",
    "        df1['user_id'] = df[:len(df1)]\n",
    "        df2['item_id'] = self.item_enc2.fit_transform(df2['item_id']) + 1\n",
    "        df2['user_id'] = df[len(df1):]\n",
    "        return df1, df2\n",
    "\n",
    "    def inverse_transform(self, df):\n",
    "        df['item_id'] = self.item_enc.inverse_transform(df['item_id']) - 1\n",
    "        df['user_id'] = self.user_enc.inverse_transform(df['user_id']) - 1\n",
    "        return df\n",
    "\n",
    "class mtlDataSet(data_utils.Dataset):\n",
    "    def __init__(self, data, args):\n",
    "        self.feature = data[0]\n",
    "        self.args = args\n",
    "        if args.mtl_task_num == 2:\n",
    "            self.label1 = data[1]\n",
    "            self.label2 = data[2]\n",
    "        else:\n",
    "            self.label = data[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.feature[index]\n",
    "        if self.args.mtl_task_num == 2:\n",
    "            label1 = self.label1[index]\n",
    "            label2 = self.label2[index]\n",
    "            return feature, label1, label2\n",
    "        else:\n",
    "            label = self.label[index]\n",
    "            return feature, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "\n",
    "class ProfileDataset(data_utils.Dataset):\n",
    "    def __init__(self, x, y, max_len, mask_token):\n",
    "        self.seqs = x\n",
    "        self.targets = y\n",
    "        self.max_len = max_len\n",
    "        self.mask_token = mask_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = self.seqs[index]\n",
    "        target = self.targets[index]\n",
    "        seq = seq + [self.mask_token]\n",
    "        seq = seq[-self.max_len:]\n",
    "        seq_len = len(seq)\n",
    "        seq_mask_len = self.max_len - seq_len\n",
    "        seq = [0] * seq_mask_len + seq\n",
    "        return torch.LongTensor(seq), torch.LongTensor([target])\n",
    "\n",
    "class BuildTrainDataset(data_utils.Dataset):\n",
    "    def __init__(self, u2seq, max_len, mask_prob, mask_token, num_items, rng):#\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.max_len = max_len\n",
    "        self.mask_prob = mask_prob\n",
    "        self.mask_token = mask_token\n",
    "        self.num_items = num_items\n",
    "        self.rng = rng\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self._getseq(user)\n",
    "\n",
    "        tokens = seq[:-1]\n",
    "        labels = seq[1:]\n",
    "        #\n",
    "        tokens = tokens[-self.max_len:]\n",
    "        labels = labels[-self.max_len:]\n",
    "\n",
    "        x_len = len(tokens)\n",
    "        y_len = len(labels)\n",
    "\n",
    "        x_mask_len = self.max_len - x_len\n",
    "        y_mask_len = self.max_len - y_len\n",
    "\n",
    "\n",
    "        tokens = [0] * x_mask_len + tokens\n",
    "        labels = [0] * y_mask_len + labels\n",
    "\n",
    "        return torch.LongTensor(tokens), torch.LongTensor(labels)\n",
    "\n",
    "    def _getseq(self, user):\n",
    "        return self.u2seq[user]\n",
    "\n",
    "class ColdDataset(data_utils.Dataset):\n",
    "    def __init__(self, x, y, max_len, mask_token):\n",
    "        self.seqs = x\n",
    "        self.targets = y\n",
    "        self.max_len = max_len\n",
    "        self.mask_token = mask_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = self.seqs[index]\n",
    "        target = self.targets[index]\n",
    "        seq = seq[-self.max_len:]\n",
    "        seq_len = len(seq)\n",
    "        seq_mask_len = self.max_len - seq_len\n",
    "        seq = [0] * seq_mask_len + seq\n",
    "        return torch.LongTensor(seq), torch.LongTensor([target])\n",
    "\n",
    "class ColdEvalDataset(data_utils.Dataset):\n",
    "    def __init__(self, x, y, max_len, mask_token, num_item):\n",
    "        self.seqs = x\n",
    "        self.targets = y\n",
    "        self.max_len = max_len\n",
    "        self.mask_token = mask_token\n",
    "        self.num_item = num_item + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = self.seqs[index]\n",
    "        target = self.targets[index]\n",
    "        labels = [0] * self.num_item\n",
    "        labels[target] = 1\n",
    "        seq = seq[-self.max_len:]\n",
    "        seq_len = len(seq)\n",
    "        seq_mask_len = self.max_len - seq_len\n",
    "        seq = [self.mask_token] * seq_mask_len + seq\n",
    "        return torch.LongTensor(seq), torch.LongTensor(labels)\n",
    "\n",
    "class pos_neg_TrainDataset(data_utils.Dataset):\n",
    "    def __init__(self, u2seq, max_len, mask_token, num_items):\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.max_len = max_len\n",
    "        self.mask_token = mask_token\n",
    "        self.num_items = num_items\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self._getseq(user)\n",
    "        tokens = seq[:-1]\n",
    "        pos = seq[1:]\n",
    "\n",
    "        tokens = tokens[-self.max_len:]\n",
    "        pos = pos[-self.max_len:]\n",
    "        seen = set(tokens)\n",
    "        seen.update(pos)\n",
    "        neg = []\n",
    "        for _ in range(len(pos)):\n",
    "            item = np.random.choice(self.num_items + 1)  #\n",
    "            while item in seen or item in neg:\n",
    "                item = np.random.choice(self.num_items + 1)  #\n",
    "            neg.append(item)\n",
    "\n",
    "        neg = neg[-self.max_len:]\n",
    "\n",
    "        x_len = len(tokens)\n",
    "        p_len = len(pos)\n",
    "        n_len = len(neg)\n",
    "\n",
    "        x_mask_len = self.max_len - x_len\n",
    "        p_mask_len = self.max_len - p_len\n",
    "        n_mask_len = self.max_len - n_len\n",
    "\n",
    "        tokens = [self.mask_token] * x_mask_len + tokens\n",
    "        pos = [self.mask_token] * p_mask_len + pos\n",
    "        neg = [self.mask_token] * n_mask_len + neg\n",
    "\n",
    "        return torch.LongTensor(tokens), torch.LongTensor(pos), torch.LongTensor(neg)#, torch.LongTensor([x_len]), torch.LongTensor([user])\n",
    "\n",
    "    def _getseq(self, user):\n",
    "        return self.u2seq[user]\n",
    "\n",
    "class CFData(data_utils.Dataset):\n",
    "    def __init__(self, features,\n",
    "            num_item, train_dict, num_ng=0, is_training=None):\n",
    "        \"\"\" Note that the labels are only useful when training, we thus\n",
    "            add them in the ng_sample() function.\n",
    "        \"\"\"\n",
    "        self.features_ps = features\n",
    "        self.num_item = num_item\n",
    "        self.train_dict = train_dict\n",
    "        self.num_ng = num_ng\n",
    "        self.is_training = is_training\n",
    "        self.labels = [0 for _ in range(len(features))]\n",
    "\n",
    "    def ng_sample(self):\n",
    "        assert self.is_training, 'no need to sampling when testing'\n",
    "\n",
    "        self.features_ng = []\n",
    "        for x, value in self.train_dict.items():\n",
    "            u = x\n",
    "            for t in range(self.num_ng * len(value)):\n",
    "                j = np.random.randint(1, self.num_item)\n",
    "                while j in value:\n",
    "                    j = np.random.randint(1, self.num_item)\n",
    "                self.features_ng.append([u, j])\n",
    "\n",
    "        labels_ps = [1 for _ in range(len(self.features_ps))]\n",
    "        labels_ng = [0 for _ in range(len(self.features_ng))]\n",
    "\n",
    "        self.features_fill = self.features_ps + self.features_ng\n",
    "        self.labels_fill = labels_ps + labels_ng\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.num_ng + 1) * len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features_fill if self.is_training \\\n",
    "                    else self.features_ps\n",
    "        labels = self.labels_fill if self.is_training \\\n",
    "                    else self.labels\n",
    "\n",
    "        user = features[idx][0]\n",
    "        item = features[idx][1]\n",
    "        label = labels[idx]\n",
    "        return user, item, label\n",
    "\n",
    "class BertTrainDataset(data_utils.Dataset):\n",
    "    def __init__(self, u2seq, max_len, mask_prob, mask_token, num_items, rng):\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.max_len = max_len\n",
    "        self.mask_prob = mask_prob\n",
    "        self.mask_token = mask_token\n",
    "        self.num_items = num_items\n",
    "        self.rng = rng\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self._getseq(user)\n",
    "\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for s in seq:\n",
    "            prob = self.rng.random()\n",
    "            if prob < self.mask_prob:\n",
    "                prob /= self.mask_prob\n",
    "\n",
    "                if prob < 0.8:\n",
    "                    tokens.append(self.mask_token)\n",
    "                elif prob < 0.9:\n",
    "                    tokens.append(self.rng.randint(1, self.num_items))\n",
    "                else:\n",
    "                    tokens.append(s)\n",
    "\n",
    "                labels.append(s)\n",
    "            else:\n",
    "                tokens.append(s)\n",
    "                labels.append(0)\n",
    "\n",
    "        tokens = tokens[-self.max_len:]\n",
    "        labels = labels[-self.max_len:]\n",
    "\n",
    "        mask_len = self.max_len - len(tokens)\n",
    "\n",
    "        tokens = [0] * mask_len + tokens\n",
    "        labels = [0] * mask_len + labels\n",
    "\n",
    "        return torch.LongTensor(tokens), torch.LongTensor(labels)\n",
    "\n",
    "    def _getseq(self, user):\n",
    "        return self.u2seq[user]\n",
    "\n",
    "class BuildEvalDataset(data_utils.Dataset):\n",
    "    def __init__(self, u2seq, u2answer, max_len, mask_token, negative_samples):\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.u2answer = u2answer\n",
    "        self.max_len = max_len\n",
    "        self.mask_token = mask_token\n",
    "        self.negative_samples = negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self.u2seq[user]\n",
    "        answer = self.u2answer[user]\n",
    "        answer = answer[-1:]\n",
    "        labels = answer\n",
    "        seq = seq + [self.mask_token]\n",
    "        seq = seq[-self.max_len:]\n",
    "        padding_len = self.max_len - len(seq)\n",
    "        seq = [self.mask_token] * padding_len + seq\n",
    "\n",
    "        return torch.LongTensor(seq), torch.LongTensor(labels)\n",
    "\n",
    "class Build_neg_EvalDataset(data_utils.Dataset):\n",
    "    def __init__(self, u2seq, u2answer, max_len, mask_token, item_count, neg_samples):\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.u2answer = u2answer\n",
    "        self.max_len = max_len\n",
    "        self.mask_token = mask_token\n",
    "        self.item_count = item_count\n",
    "        self.neg_samples = neg_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self.u2seq[user]\n",
    "        answer = self.u2answer[user]\n",
    "        answer = answer[-1:]\n",
    "        negs = self.neg_samples[user]\n",
    "\n",
    "        candidates = answer + negs\n",
    "        labels = [1] * len(answer) + [0] * len(negs)\n",
    "        seq = seq + [self.mask_token]\n",
    "        seq = seq[-self.max_len:]\n",
    "        padding_len = self.max_len - len(seq)\n",
    "        seq = [self.mask_token] * padding_len + seq\n",
    "\n",
    "        return torch.LongTensor(seq), torch.LongTensor(candidates), torch.LongTensor(labels)\n",
    "\n",
    "class Build_full_EvalDataset(data_utils.Dataset):\n",
    "    def __init__(self, u2seq, u2answer, max_len, mask_token, num_item):\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        self.u2answer = u2answer\n",
    "        self.max_len = max_len\n",
    "        self.mask_token = mask_token\n",
    "        self.num_item = num_item + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self.u2seq[user][:-1]\n",
    "        answer = self.u2answer[user]\n",
    "        answer = answer[-1:][0]\n",
    "\n",
    "\n",
    "        labels = [0] * self.num_item\n",
    "        labels[answer] = 1\n",
    "        seq = seq + [self.mask_token]\n",
    "        seq = seq[-self.max_len:]\n",
    "        padding_len = self.max_len - len(seq)\n",
    "        seq = [self.mask_token] * padding_len + seq\n",
    "\n",
    "        return torch.LongTensor(seq), torch.LongTensor(labels)\n",
    "\n",
    "class new_Build_full_EvalDataset(data_utils.Dataset):\n",
    "    def __init__(self, u2seq, max_len, mask_token, num_item):\n",
    "        self.u2seq = u2seq\n",
    "        self.users = sorted(self.u2seq.keys())\n",
    "        # self.u2answer = u2answer\n",
    "        self.max_len = max_len\n",
    "        self.mask_token = mask_token\n",
    "        self.num_item = num_item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.users[index]\n",
    "        seq = self.u2seq[user][:-2]\n",
    "        answer = self.u2seq[user][-1:][0]\n",
    "\n",
    "        labels = [0] * self.num_item\n",
    "        labels[answer] = 1\n",
    "        seq = seq + [self.mask_token]\n",
    "        seq = seq[-self.max_len:]\n",
    "        padding_len = self.max_len - len(seq)\n",
    "        seq = [self.mask_token] * padding_len + seq\n",
    "\n",
    "        return torch.LongTensor(seq), torch.LongTensor(labels)\n",
    "\n",
    "class AEDataset(data_utils.Dataset):\n",
    "    def __init__(self, train_set, yield_col='user_id'):\n",
    "        \"\"\"\n",
    "        covert user in train_set to array-like <u> / <i> for AutoEncoder-like algorithms\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_set : pd.DataFrame\n",
    "            training set\n",
    "        yield_col : string\n",
    "            column name used to generate array\n",
    "        \"\"\"\n",
    "        super(AEDataset, self).__init__()\n",
    "        self.data = list(train_set[yield_col].unique())\n",
    "        self.data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "class VAEDataset(data_utils.Dataset):\n",
    "    def __init__(self, train_set):\n",
    "        \"\"\"\n",
    "        covert user in train_set to array-like <u> / <i> for AutoEncoder-like algorithms\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_set : pd.DataFrame\n",
    "            training set\n",
    "        yield_col : string\n",
    "            column name used to generate array\n",
    "        \"\"\"\n",
    "        super(VAEDataset, self).__init__()\n",
    "        self.data = train_set\n",
    "        self.data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "\n",
    "class Cf_valDataset(data_utils.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(Cf_valDataset, self).__init__()\n",
    "        self.user = data\n",
    "        # self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user = self.user[index]\n",
    "        return torch.tensor(user)#, torch.tensor(self.data[user])\n",
    "\n",
    "\n",
    "class CandidatesDataset(data_utils.Dataset):\n",
    "    def __init__(self, ucands):\n",
    "        super(CandidatesDataset, self).__init__()\n",
    "        self.data = ucands\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index][0]), torch.tensor(self.data[index][1])\n",
    "\n",
    "def get_train_loader(dataset, args):\n",
    "    if args['is_parallel']:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['train_batch_size'], sampler=DistributedSampler(dataset))\n",
    "    else:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['train_batch_size'], shuffle=True, pin_memory=True)\n",
    "    return dataloader\n",
    "\n",
    "def get_val_loader(dataset, args):\n",
    "    if args['is_parallel']:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['val_batch_size'], sampler=DistributedSampler(dataset))\n",
    "    else:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['val_batch_size'], shuffle=False, pin_memory=True)\n",
    "    return dataloader\n",
    "\n",
    "def get_test_loader(dataset, args):\n",
    "    if args['is_parallel']:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['test_batch_size'], sampler=DistributedSampler(dataset))\n",
    "    else:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['test_batch_size'], shuffle=False, pin_memory=True)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71056702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T07:53:17.915066Z",
     "iopub.status.busy": "2023-07-25T07:53:17.914710Z",
     "iopub.status.idle": "2023-07-25T07:53:17.921472Z",
     "shell.execute_reply": "2023-07-25T07:53:17.920479Z",
     "shell.execute_reply.started": "2023-07-25T07:53:17.915026Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_sampler(train_data, val_data, test_data, user_count, item_count, args):\n",
    "    if args.sample == 'random':\n",
    "        return RandomNegativeSampler(train_data, val_data, test_data, user_count, item_count, args.negsample_size, args.seed, args.negsample_savefolder)\n",
    "    elif args.sample == 'popular':\n",
    "        return PopularNegativeSampler(train_data, val_data, test_data, user_count, item_count, args.negsample_size, args.seed, args.negsample_savefolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd35c63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T07:53:17.923998Z",
     "iopub.status.busy": "2023-07-25T07:53:17.923251Z",
     "iopub.status.idle": "2023-07-25T07:53:17.933474Z",
     "shell.execute_reply": "2023-07-25T07:53:17.932498Z",
     "shell.execute_reply.started": "2023-07-25T07:53:17.923964Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(args):\n",
    "    path = args['dataset_path']\n",
    "    rng = random.Random(args['seed'])\n",
    "    \n",
    "    data, user_count, vocab_size, item_count = colddataset(args['item_min'], args)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data.source.values.tolist(),\n",
    "                                                            data.target.values.tolist(),\n",
    "                                                            test_size=0.2, random_state=args['seed'])\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=args['seed'])\n",
    "\n",
    "    args['num_users'] = user_count\n",
    "    args['num_items'] = item_count\n",
    "    args['num_embedding'] = vocab_size\n",
    "\n",
    "    train_dataset, valid_dataset = ColdDataset(x_train, y_train, args['max_len'], args['pad_token']), ColdEvalDataset(\n",
    "        x_val, y_val, args['max_len'], args['pad_token'], args['num_items'])\n",
    "    test_dataset = ColdEvalDataset(x_test, y_test, args['max_len'], args['pad_token'], args['num_items'])\n",
    "    train_dataloader = get_train_loader(train_dataset, args)\n",
    "    valid_dataloader = get_val_loader(valid_dataset, args)\n",
    "    test_dataloader = get_test_loader(test_dataset, args)\n",
    "    return train_dataloader, valid_dataloader, test_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8584d1",
   "metadata": {},
   "source": [
    "# Peter4ColdStart Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6386bfbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T07:53:17.936778Z",
     "iopub.status.busy": "2023-07-25T07:53:17.936469Z",
     "iopub.status.idle": "2023-07-25T07:53:17.986617Z",
     "shell.execute_reply": "2023-07-25T07:53:17.985683Z",
     "shell.execute_reply.started": "2023-07-25T07:53:17.936755Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.init import uniform_, xavier_normal_, constant_, normal_\n",
    "\n",
    "class Peter4Coldstart(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(Peter4Coldstart, self).__init__()\n",
    "\n",
    "        # load parameters info\n",
    "        self.embedding_size = args['embedding_size']\n",
    "        self.residual_channels = args['embedding_size'] \n",
    "        self.block_num = args['block_num']\n",
    "        self.dilations = args['dilations'] * self.block_num\n",
    "        self.kernel_size = args['kernel_size']\n",
    "        self.output_dim = args['num_items']\n",
    "        self.vocab_size = args['num_embedding']\n",
    "        self.is_mp = args['is_mp']\n",
    "\n",
    "        self.pad_token = args['pad_token']\n",
    "\n",
    "        # define layers and loss\n",
    "        self.item_embedding = nn.Embedding(self.vocab_size+1, self.embedding_size, padding_idx=self.pad_token)\n",
    "    \n",
    "        # residual blocks\n",
    "        rb = [\n",
    "            ResidualBlock_b_2mp_parallel(\n",
    "                self.residual_channels, self.residual_channels, kernel_size=self.kernel_size, dilation=dilation, is_mp=self.is_mp\n",
    "            ) for dilation in self.dilations\n",
    "        ]\n",
    "        self.residual_blocks = nn.Sequential(*rb)\n",
    "\n",
    "        # fully-connected layer\n",
    "        self.final_layer = nn.Linear(self.residual_channels, self.output_dim+1)\n",
    "        # parameters initialization\n",
    "        # self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            stdv = np.sqrt(1. / (self.output_dim+1))\n",
    "            uniform_(module.weight.data, -stdv, stdv)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            # xavier_normal_(module.weight.data)\n",
    "            normal_(module.weight.data, 0.0, 0.1)\n",
    "            if module.bias is not None:\n",
    "                constant_(module.bias.data, 0.1)\n",
    "\n",
    "    def forward(self, item_seq):#, pos, neg\n",
    "        item_seq_emb = self.item_embedding(item_seq)  # [batch_size, seq_len, embed_size]\n",
    "        # Residual locks\n",
    "        dilate_outputs = self.residual_blocks(item_seq_emb)\n",
    "        seq_output = self.final_layer(dilate_outputs)  # [batch_size, embedding_size]hidden\n",
    "        return seq_output\n",
    "\n",
    "    def predict(self, item_seq, item):\n",
    "        item_seq_emb = self.item_embedding(item_seq)  # [batch_size, seq_len, embed_size]\n",
    "        dilate_outputs = self.residual_blocks(item_seq_emb)\n",
    "        item_embs = self.item_embedding(item)\n",
    "        logits = dilate_outputs.matmul(item_embs.transpose(1, 2))\n",
    "        logits = logits.mean(1)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class mp(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(mp, self).__init__()\n",
    "        self.hidden_size = int(channel / 4)\n",
    "        self.conv1 = nn.Conv1d(channel, self.hidden_size, 1)\n",
    "        self.conv2 = nn.Conv1d(self.hidden_size, channel, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "class ResidualBlock_a(nn.Module):\n",
    "    r\"\"\"\n",
    "    Residual block (a) in the paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, dilation=None):\n",
    "        super(ResidualBlock_a, self).__init__()\n",
    "\n",
    "        half_channel = out_channel // 2\n",
    "        self.ln1 = nn.LayerNorm(out_channel, eps=1e-8)\n",
    "        self.conv1 = nn.Conv2d(in_channel, half_channel, kernel_size=(1, 1), padding=0)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(half_channel, eps=1e-8)\n",
    "        self.conv2 = nn.Conv2d(half_channel, half_channel, kernel_size=(1, kernel_size), padding=0, dilation=dilation)\n",
    "\n",
    "        self.ln3 = nn.LayerNorm(half_channel, eps=1e-8)\n",
    "        self.conv3 = nn.Conv2d(half_channel, out_channel, kernel_size=(1, 1), padding=0)\n",
    "\n",
    "        self.dilation = dilation\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def forward(self, x):  # x: [batch_size, seq_len, embed_size]\n",
    "\n",
    "        out = F.relu(self.ln1(x))\n",
    "        print(2.1)\n",
    "        print(out.shape)\n",
    "        out = out.permute(0, 2, 1).unsqueeze(2)\n",
    "        print(2.2)\n",
    "        print(out.shape)\n",
    "        out = self.conv1(out).squeeze(2).permute(0, 2, 1)\n",
    "        print(2.3)\n",
    "        out2 = F.relu(self.ln2(out))\n",
    "        out2 = self.conv_pad(out2, self.dilation)\n",
    "        out2 = self.conv2(out2).squeeze(2).permute(0, 2, 1)\n",
    "\n",
    "        out3 = F.relu(self.ln3(out2))\n",
    "        out3 = out3.permute(0, 2, 1).unsqueeze(2)\n",
    "        out3 = self.conv3(out3).squeeze(2).permute(0, 2, 1)\n",
    "        return out3 + x\n",
    "\n",
    "    def conv_pad(self, x, dilation):  # x: [batch_size, seq_len, embed_size]\n",
    "        r\"\"\" Dropout-mask: To avoid the future information leakage problem, this paper proposed a masking-based dropout\n",
    "        trick for the 1D dilated convolution to prevent the network from seeing the future items.\n",
    "        Also the One-dimensional transformation is completed in this function.\n",
    "        \"\"\"\n",
    "        inputs_pad = x.permute(0, 2, 1)  # [batch_size, embed_size, seq_len]\n",
    "        inputs_pad = inputs_pad.unsqueeze(2)  # [batch_size, embed_size, 1, seq_len]\n",
    "        pad = nn.ZeroPad2d(((self.kernel_size - 1) * dilation, 0, 0, 0))\n",
    "        # padding operation  args(left,right,top,bottom)\n",
    "        inputs_pad = pad(inputs_pad)  # [batch_size, embed_size, 1, seq_len+(self.kernel_size-1)*dilations]\n",
    "        return inputs_pad\n",
    "\n",
    "    \n",
    "class ResidualBlock_b_2mp_parallel(nn.Module):\n",
    "    r\"\"\"\n",
    "    Residual block (b) in the paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, dilation=None, is_mp=False):\n",
    "        super(ResidualBlock_b_2mp_parallel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=(1, kernel_size), padding=0, dilation=dilation)\n",
    "        self.ln1 = nn.LayerNorm(out_channel, eps=1e-8)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=(1, kernel_size), padding=0, dilation=dilation * 2)\n",
    "        self.ln2 = nn.LayerNorm(out_channel, eps=1e-8)\n",
    "        \n",
    "        self.dilation = dilation\n",
    "        self.kernel_size = kernel_size\n",
    "        self.is_mp = is_mp\n",
    "        self.rez = nn.Parameter(torch.FloatTensor([1]))\n",
    "        if self.is_mp:\n",
    "            self.mp1 = mp(in_channel)\n",
    "            self.mp2 = mp(in_channel)\n",
    "\n",
    "    def forward(self, x):  # x: [batch_size, seq_len, embed_size]\n",
    "        x_pad = self.conv_pad(x, self.dilation)  # [batch_size, embeding_size, 1, seq_len+(self.kernel_size-1)*dilations]\n",
    "        out = self.conv1(x_pad).squeeze(2).permute(0, 2, 1)\n",
    "        # [batch_size, seq_len+(self.kernel_size-1)*dilations-kernel_size+1, embed_size]\n",
    "        \n",
    "        if self.is_mp:\n",
    "            mp_out = self.mp1(x)\n",
    "            out = mp_out + out\n",
    "        out = F.relu(self.ln1(out))\n",
    "        out_pad = self.conv_pad(out, self.dilation * 2)\n",
    "        out2 = self.conv2(out_pad).squeeze(2).permute(0, 2, 1)\n",
    "        if self.is_mp:\n",
    "            mp_out2 = self.mp2(out)\n",
    "            out2 = mp_out2 + out2\n",
    "        out2 = F.relu(self.ln2(out2))\n",
    "        return out2 * self.rez + x\n",
    "\n",
    "    def conv_pad(self, x, dilation):\n",
    "        r\"\"\" Dropout-mask: To avoid the future information leakage problem, this paper proposed a masking-based dropout\n",
    "        trick for the 1D dilated convolution to prevent the network from seeing the future items.\n",
    "        Also the One-dimensional transformation is completed in this function.\n",
    "        \"\"\"\n",
    "        inputs_pad = x.permute(0, 2, 1)\n",
    "        inputs_pad = inputs_pad.unsqueeze(2)\n",
    "        pad = nn.ZeroPad2d(((self.kernel_size - 1) * dilation, 0, 0, 0))\n",
    "        inputs_pad = pad(inputs_pad)\n",
    "        return inputs_pad\n",
    "\n",
    "class ResidualBlock_b_2mp_serial(nn.Module):\n",
    "    r\"\"\"\n",
    "    Residual block (b) in the paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, dilation=None, is_mp=False):\n",
    "        super(ResidualBlock_b_2mp_serial, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=(1, kernel_size), padding=0, dilation=dilation)\n",
    "        self.ln1 = nn.LayerNorm(out_channel, eps=1e-8)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=(1, kernel_size), padding=0, dilation=dilation * 2)\n",
    "        self.ln2 = nn.LayerNorm(out_channel, eps=1e-8)\n",
    "        # self.mp = mp(in_channel)\n",
    "        self.dilation = dilation\n",
    "        self.kernel_size = kernel_size\n",
    "        self.is_mp = is_mp\n",
    "        if self.is_mp:\n",
    "            self.mp1 = mp(in_channel)\n",
    "            self.mp2 = mp(in_channel)\n",
    "\n",
    "    def forward(self, x):  # x: [batch_size, seq_len, embed_size]\n",
    "        x_pad = self.conv_pad(x, self.dilation)  # [batch_size, embed_size, 1, seq_len+(self.kernel_size-1)*dilations]\n",
    "        out = self.conv1(x_pad).squeeze(2).permute(0, 2, 1)\n",
    "        # [batch_size, seq_len+(self.kernel_size-1)*dilations-kernel_size+1, embed_size]\n",
    "        if self.is_mp:\n",
    "            mp_out = self.mp1(x)\n",
    "            out = mp_out\n",
    "        out = F.relu(self.ln1(out))\n",
    "        out_pad = self.conv_pad(out, self.dilation * 2)\n",
    "        out2 = self.conv2(out_pad).squeeze(2).permute(0, 2, 1)\n",
    "        if self.is_mp:\n",
    "            mp_out2 = self.mp2(out)\n",
    "            out2 = mp_out2\n",
    "        out2 = F.relu(self.ln2(out2))\n",
    "        return out2 + x\n",
    "\n",
    "    def conv_pad(self, x, dilation):\n",
    "        r\"\"\" Dropout-mask: To avoid the future information leakage problem, this paper proposed a masking-based dropout\n",
    "        trick for the 1D dilated convolution to prevent the network from seeing the future items.\n",
    "        Also the One-dimensional transformation is completed in this function.\n",
    "        \"\"\"\n",
    "        inputs_pad = x.permute(0, 2, 1)\n",
    "        inputs_pad = inputs_pad.unsqueeze(2)\n",
    "        pad = nn.ZeroPad2d(((self.kernel_size - 1) * dilation, 0, 0, 0))\n",
    "        inputs_pad = pad(inputs_pad)\n",
    "        return inputs_pad\n",
    "\n",
    "class ResidualBlock_b_mp_serial(nn.Module):\n",
    "    r\"\"\"\n",
    "    Residual block (b) in the paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, dilation=None, is_mp=False):\n",
    "        super(ResidualBlock_b_mp_serial, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=(1, kernel_size), padding=0, dilation=dilation)\n",
    "        self.ln1 = nn.LayerNorm(out_channel, eps=1e-8)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=(1, kernel_size), padding=0, dilation=dilation * 2)\n",
    "        self.ln2 = nn.LayerNorm(out_channel, eps=1e-8)\n",
    "        self.dilation = dilation\n",
    "        self.kernel_size = kernel_size\n",
    "        self.is_mp = is_mp\n",
    "        if self.is_mp:\n",
    "            self.mp = mp(in_channel)\n",
    "\n",
    "    def forward(self, x):  # x: [batch_size, seq_len, embed_size]\n",
    "        x_pad = self.conv_pad(x, self.dilation)  # [batch_size, embed_size, 1, seq_len+(self.kernel_size-1)*dilations]\n",
    "        out = self.conv1(x_pad).squeeze(2).permute(0, 2, 1)\n",
    "        # [batch_size, seq_len+(self.kernel_size-1)*dilations-kernel_size+1, embed_size]\n",
    "        out = F.relu(self.ln1(out))\n",
    "        out_pad = self.conv_pad(out, self.dilation * 2)\n",
    "        out2 = self.conv2(out_pad).squeeze(2).permute(0, 2, 1)\n",
    "        out2 = F.relu(self.ln2(out2))\n",
    "        if self.is_mp:\n",
    "            mp_out2 = self.mp(out)\n",
    "            out2 = mp_out2\n",
    "        return out2 + x\n",
    "\n",
    "    def conv_pad(self, x, dilation):\n",
    "        r\"\"\" Dropout-mask: To avoid the future information leakage problem, this paper proposed a masking-based dropout\n",
    "        trick for the 1D dilated convolution to prevent the network from seeing the future items.\n",
    "        Also the One-dimensional transformation is completed in this function.\n",
    "        \"\"\"\n",
    "        inputs_pad = x.permute(0, 2, 1)\n",
    "        inputs_pad = inputs_pad.unsqueeze(2)\n",
    "        pad = nn.ZeroPad2d(((self.kernel_size - 1) * dilation, 0, 0, 0))\n",
    "        inputs_pad = pad(inputs_pad)\n",
    "        return inputs_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62ff1efa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T07:53:17.988530Z",
     "iopub.status.busy": "2023-07-25T07:53:17.988143Z",
     "iopub.status.idle": "2023-07-25T07:53:18.000612Z",
     "shell.execute_reply": "2023-07-25T07:53:17.999678Z",
     "shell.execute_reply.started": "2023-07-25T07:53:17.988482Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(args, linear_feature_columns=None, dnn_feature_columns=None, history_feature_list=None):\n",
    "    return Peter4Coldstart(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3bc326f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T07:53:18.003742Z",
     "iopub.status.busy": "2023-07-25T07:53:18.003337Z",
     "iopub.status.idle": "2023-07-25T07:53:18.011613Z",
     "shell.execute_reply": "2023-07-25T07:53:18.010552Z",
     "shell.execute_reply.started": "2023-07-25T07:53:18.003709Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed, re=True):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    if re:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    else:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7ee0a",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c4ada29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T07:53:18.016011Z",
     "iopub.status.busy": "2023-07-25T07:53:18.015693Z",
     "iopub.status.idle": "2023-07-25T07:53:18.026841Z",
     "shell.execute_reply": "2023-07-25T07:53:18.025895Z",
     "shell.execute_reply.started": "2023-07-25T07:53:18.015985Z"
    }
   },
   "outputs": [],
   "source": [
    "def Sequence_full_Validate(epoch, model, dataloader, writer, args, test=False):\n",
    "    print(\"+\" * 20, \"Valid Epoch {}\".format(epoch + 1), \"+\" * 20)\n",
    "    model.eval()\n",
    "    avg_metrics = {}\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        tqdm_dataloader = dataloader\n",
    "        for data in tqdm_dataloader:\n",
    "            data = [x.to(args['device']) for x in data]\n",
    "            seqs, labels = data\n",
    "            if test:\n",
    "                scores = model.predict(seqs)\n",
    "            else:\n",
    "                scores = model(seqs)\n",
    "            scores = scores.mean(1)\n",
    "            metrics = recalls_and_ndcgs_for_ks(scores, labels, args['metric_ks'], args)\n",
    "            i += 1\n",
    "            for key, value in metrics.items():\n",
    "                if key not in avg_metrics:\n",
    "                    avg_metrics[key] = value\n",
    "                else:\n",
    "                    avg_metrics[key] += value\n",
    "    for key, value in avg_metrics.items():\n",
    "        avg_metrics[key] = value / i\n",
    "    print(avg_metrics)\n",
    "    for k in sorted(args['metric_ks'], reverse=True):\n",
    "        writer.add_scalar('Train/NDCG@{}'.format(k), avg_metrics['NDCG@%d' % k], epoch)\n",
    "    return avg_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6beb811",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T07:53:18.028688Z",
     "iopub.status.busy": "2023-07-25T07:53:18.028333Z",
     "iopub.status.idle": "2023-07-25T07:53:18.041303Z",
     "shell.execute_reply": "2023-07-25T07:53:18.040358Z",
     "shell.execute_reply.started": "2023-07-25T07:53:18.028656Z"
    }
   },
   "outputs": [],
   "source": [
    "def recalls_and_ndcgs_for_ks(scores, labels, ks, args):\n",
    "    metrics = {}\n",
    "\n",
    "    answer_count = labels.sum(1)\n",
    "    answer_count_float = answer_count.float()\n",
    "    labels_float = labels.float()\n",
    "    rank = (-scores).argsort(dim=0)\n",
    "    cut = rank\n",
    "    for k in sorted(ks, reverse=True):\n",
    "       cut = cut[:, :k]\n",
    "       hits = torch.gather(labels_float,1,cut)\n",
    "       metrics['Recall@%d' % k] = (hits.sum(1) / answer_count_float).mean().item()\n",
    "\n",
    "       position = torch.arange(2, 2+k)\n",
    "       weights = 1 / torch.log2(position.float()).to(args['device'])\n",
    "       dcg = (hits * weights).sum(1)\n",
    "       idcg = torch.Tensor([weights[:min(n, k)].sum() for n in answer_count]).to(args['device'])\n",
    "       ndcg = (dcg / idcg).mean()\n",
    "       metrics['NDCG@%d' % k] = ndcg\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ac6a716",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T07:53:18.043213Z",
     "iopub.status.busy": "2023-07-25T07:53:18.042836Z",
     "iopub.status.idle": "2023-07-25T07:53:18.052786Z",
     "shell.execute_reply": "2023-07-25T07:53:18.051661Z",
     "shell.execute_reply.started": "2023-07-25T07:53:18.043181Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def SequenceTrainer(epoch, model, dataloader, optimizer, writer, args): #schedular,\n",
    "    print(\"+\" * 20, \"Train Epoch {}\".format(epoch + 1), \"+\" * 20)\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        data = [x.to(args['device']) for x in data]\n",
    "        seqs, labels = data\n",
    "        logits = model(seqs) # B x T x V\n",
    "        logits = logits.mean(1)\n",
    "        labels = labels.view(-1)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.detach().cpu().item()\n",
    "    writer.add_scalar('Train/loss', running_loss / len(dataloader), epoch)\n",
    "    print(\"Training CE Loss: {:.5f}\".format(running_loss / len(dataloader)))\n",
    "    return optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeb55ad8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T07:53:18.054934Z",
     "iopub.status.busy": "2023-07-25T07:53:18.054445Z",
     "iopub.status.idle": "2023-07-25T07:53:18.070609Z",
     "shell.execute_reply": "2023-07-25T07:53:18.069826Z",
     "shell.execute_reply.started": "2023-07-25T07:53:18.054901Z"
    }
   },
   "outputs": [],
   "source": [
    "def SeqTrain(epochs, model, train_loader, val_loader, writer, args):\n",
    "    if args['is_pretrain'] == 0:\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                     lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "    model = model.to(args['device'])\n",
    "    if args['is_parallel']:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model,  find_unused_parameters=True,device_ids=[args['local_rank']], output_device=[args['local_rank']])\n",
    "    best_metric = 0\n",
    "    all_time = 0\n",
    "    val_all_time = 0\n",
    "    for epoch in range(epochs):\n",
    "        since = time.time()\n",
    "        optimizer = SequenceTrainer(epoch, model, train_loader, optimizer, writer, args)\n",
    "        tmp = time.time() - since\n",
    "        print('one epoch train:', tmp)\n",
    "        all_time += tmp\n",
    "        val_since = time.time()\n",
    "        metrics = Sequence_full_Validate(epoch, model, val_loader, writer, args)\n",
    "        val_tmp = time.time() - val_since\n",
    "        print('one epoch val:', val_tmp)\n",
    "        val_all_time += val_tmp\n",
    "        if args['is_pretrain'] == 0 and 'acc' in args['task_name']:\n",
    "            if metrics['NDCG@20'] >= 0.0193:\n",
    "                break\n",
    "        i = 1\n",
    "        current_metric = metrics['NDCG@5']\n",
    "        if best_metric <= current_metric:\n",
    "            best_metric = current_metric\n",
    "            best_model = deepcopy(model)\n",
    "            state_dict = model.state_dict()\n",
    "            if 'life' in args['task_name']:\n",
    "                torch.save(state_dict, os.path.join(args['save_path'],\n",
    "                                                         '{}_{}_seed{}_task_{}_best_model.pth'.format('sequence',\n",
    "                                                                                                      args['model_name'],\n",
    "                                                                                                      args['seed'],\n",
    "                                                                                                      args['task'])))\n",
    "            else:\n",
    "                torch.save(state_dict, os.path.join(args['save_path'], '{}_{}_seed{}_is_pretrain_{}_best_model_lr{}_wd{}_block{}_hd{}_emb{}.pth'.format(args['task_name'], args['model_name'], args['seed'], args['is_pretrain'],\n",
    "                                                                                                                              args['lr'], args['weight_decay'], args['block_num'], args['hidden_size'], args['embedding_size'])))\n",
    "        else:\n",
    "            i += 1\n",
    "            if i == 10:\n",
    "                print('early stop!')\n",
    "                break\n",
    "    print('train_time:', all_time)\n",
    "    print('val_time:', val_all_time)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df8dfb",
   "metadata": {},
   "source": [
    "# Model Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70a9e2",
   "metadata": {},
   "source": [
    "## Trial 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04364746",
   "metadata": {},
   "source": [
    "- We will train this trial with (learning rate = 0.001, epochs = 10 and batch_size = 150).\n",
    "- We will train the model with a sample of sbr_data_1M (1000000 samples), as it crashes a larger number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b1050b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T07:53:18.072660Z",
     "iopub.status.busy": "2023-07-25T07:53:18.071998Z",
     "iopub.status.idle": "2023-07-25T07:53:22.505992Z",
     "shell.execute_reply": "2023-07-25T07:53:22.505041Z",
     "shell.execute_reply.started": "2023-07-25T07:53:18.072626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============cold_start=============\n",
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting user ids and item ids in DataFrame ==========\n",
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting item ids in DataFrame ==========\n",
      "+++user_history+++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 38/38 [00:00<00:00, 51265.21it/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"./checkpoint_trial1\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    \n",
    "args = {\n",
    "    'seed': 0,\n",
    "    'task_name': '',\n",
    "    'task_num': 4,\n",
    "    'dataset_path': '',\n",
    "    'pretrain_path': '',\n",
    "    'source_path': '',\n",
    "    'target_path': '',\n",
    "    'train_batch_size': 150,\n",
    "    'val_batch_size': 150,\n",
    "    'test_batch_size': 150,\n",
    "    'sample': 'random',\n",
    "    'negsample_savefolder': './data/neg_data/',\n",
    "    'negsample_size': 99,\n",
    "    'max_len': 20,\n",
    "    'item_min': 10,\n",
    "    'save_path': checkpoint_dir,\n",
    "    'task': -1,\n",
    "    'valid_rate': 100,\n",
    "    'model_name': 'Peter4Coldstart',\n",
    "    'epochs': 10,\n",
    "    're_epochs': 20,\n",
    "    'lr': 0.001,\n",
    "    'device': 'cuda',\n",
    "    'is_parallel': False,\n",
    "    'local_rank': None,\n",
    "    'num_gpu': 1,\n",
    "    'weight_decay': 0.0,\n",
    "    'decay_step': 5,\n",
    "    'gamma': 0.5,\n",
    "    'num_users': 1,\n",
    "    'num_items': 1,\n",
    "    'num_embedding': 1,\n",
    "    'num_labels': 1,\n",
    "    'k': 20,\n",
    "    'metric_ks': [5, 20],\n",
    "    'best_metric': 'NDCG@10',\n",
    "    'hidden_size': 128,\n",
    "    'block_num': 2,\n",
    "    'num_groups': 4,\n",
    "    'num_heads': 4,\n",
    "    'dropout': 0.3,\n",
    "    'bert_mask_prob': 0.3,\n",
    "    'factor_num': 128,\n",
    "    'embedding_size': 128,\n",
    "    'dilations': [1, 4],\n",
    "    'kernel_size': 3,\n",
    "    'is_mp': False,\n",
    "    'pad_token': 0,\n",
    "    'temp': 7,\n",
    "    'l2_emb': 0.0,\n",
    "    'mtl_task_num': 1,\n",
    "    'test_method': 'ufo',\n",
    "    'val_method': 'ufo',\n",
    "    'test_size': 0.1,\n",
    "    'val_size': 0.1111,\n",
    "    'cand_num': 100,\n",
    "    'sample_method': 'high-pop',\n",
    "    'sample_ratio': 0.3,\n",
    "    'num_ng': 4,\n",
    "    'loss_type': 'BPR',\n",
    "    'init_method': 'default',\n",
    "    'optimizer': 'default',\n",
    "    'early_stop': True,\n",
    "    'reg_1': 0.0,\n",
    "    'reg_2': 0.0,\n",
    "    'context_window': 2,\n",
    "    'rho': 0.5,\n",
    "    'node_dropout': 0.1,\n",
    "    'mess_dropout': 0.1,\n",
    "    'hidden_size_list': [128, 128],\n",
    "    'latent_dim': 128,\n",
    "    'anneal_cap': 0.2,\n",
    "    'total_anneal_steps': 1000,\n",
    "    'kd': False,\n",
    "    'alpha': 0.4,\n",
    "    'add_num_times': 2,\n",
    "    'is_pretrain': 1,\n",
    "    'user_profile': 'gender',\n",
    "    'prun_rate': 0,\n",
    "    'll_max_itemnum': 0,\n",
    "    'lifelong_eval': True,\n",
    "    'task1_out': 0,\n",
    "    'task2_out': 0,\n",
    "    'task3_out': 0,\n",
    "    'task4_out': 0,\n",
    "    'eval': True,\n",
    "    'ch':True,\n",
    "}\n",
    "if args['is_parallel']:\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    torch.cuda.set_device(args['local_rank'])\n",
    "device = torch.device(args['device'])\n",
    "set_seed(args['seed'])\n",
    "writer = SummaryWriter()\n",
    "print('=============cold_start=============')\n",
    "args['source_path'] = '/kaggle/input/tenrecdatasets/sbr_data_1M_sampled.csv'\n",
    "args['target_path'] = '/kaggle/input/tenrecdata/cold_data_0.7.csv'\n",
    "train_loader, val_loader, test_loader = get_data(args) #, user_noclick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68e28870",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T07:53:22.508091Z",
     "iopub.status.busy": "2023-07-25T07:53:22.507498Z",
     "iopub.status.idle": "2023-07-25T07:53:34.449958Z",
     "shell.execute_reply": "2023-07-25T07:53:34.448962Z",
     "shell.execute_reply.started": "2023-07-25T07:53:22.508053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrain\n",
      "++++++++++++++++++++ Train Epoch 1 ++++++++++++++++++++\n",
      "Training CE Loss: 7.18329\n",
      "one epoch train: 5.381599187850952\n",
      "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
      "{'Recall@20': 0.014084506779909134, 'NDCG@20': tensor(0.0035, device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n",
      "one epoch val: 0.09839940071105957\n",
      "++++++++++++++++++++ Train Epoch 2 ++++++++++++++++++++\n",
      "Training CE Loss: 6.61068\n",
      "one epoch train: 0.10537266731262207\n",
      "++++++++++++++++++++ Valid Epoch 2 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0, 'NDCG@20': tensor(0., device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n",
      "one epoch val: 0.021085023880004883\n",
      "++++++++++++++++++++ Train Epoch 3 ++++++++++++++++++++\n",
      "Training CE Loss: 6.38680\n",
      "one epoch train: 0.10496187210083008\n",
      "++++++++++++++++++++ Valid Epoch 3 ++++++++++++++++++++\n",
      "{'Recall@20': 0.014084506779909134, 'NDCG@20': tensor(0.0036, device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n",
      "one epoch val: 0.0217132568359375\n",
      "++++++++++++++++++++ Train Epoch 4 ++++++++++++++++++++\n",
      "Training CE Loss: 6.26569\n",
      "one epoch train: 0.10645771026611328\n",
      "++++++++++++++++++++ Valid Epoch 4 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0, 'NDCG@20': tensor(0., device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n",
      "one epoch val: 0.023035526275634766\n",
      "++++++++++++++++++++ Train Epoch 5 ++++++++++++++++++++\n",
      "Training CE Loss: 6.17092\n",
      "one epoch train: 0.1049659252166748\n",
      "++++++++++++++++++++ Valid Epoch 5 ++++++++++++++++++++\n",
      "{'Recall@20': 0.014084506779909134, 'NDCG@20': tensor(0.0089, device='cuda:0'), 'Recall@5': 0.014084506779909134, 'NDCG@5': tensor(0.0089, device='cuda:0')}\n",
      "one epoch val: 0.02299356460571289\n",
      "++++++++++++++++++++ Train Epoch 6 ++++++++++++++++++++\n",
      "Training CE Loss: 6.11014\n",
      "one epoch train: 0.367936372756958\n",
      "++++++++++++++++++++ Valid Epoch 6 ++++++++++++++++++++\n",
      "{'Recall@20': 0.028169013559818268, 'NDCG@20': tensor(0.0073, device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n",
      "one epoch val: 0.021799087524414062\n",
      "++++++++++++++++++++ Train Epoch 7 ++++++++++++++++++++\n",
      "Training CE Loss: 6.03723\n",
      "one epoch train: 0.10197782516479492\n",
      "++++++++++++++++++++ Valid Epoch 7 ++++++++++++++++++++\n",
      "{'Recall@20': 0.014084506779909134, 'NDCG@20': tensor(0.0036, device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n",
      "one epoch val: 0.021941423416137695\n",
      "++++++++++++++++++++ Train Epoch 8 ++++++++++++++++++++\n",
      "Training CE Loss: 5.96480\n",
      "one epoch train: 0.10626721382141113\n",
      "++++++++++++++++++++ Valid Epoch 8 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0, 'NDCG@20': tensor(0., device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n",
      "one epoch val: 0.02197718620300293\n",
      "++++++++++++++++++++ Train Epoch 9 ++++++++++++++++++++\n",
      "Training CE Loss: 5.88081\n",
      "one epoch train: 0.1016385555267334\n",
      "++++++++++++++++++++ Valid Epoch 9 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0, 'NDCG@20': tensor(0., device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n",
      "one epoch val: 0.0234529972076416\n",
      "++++++++++++++++++++ Train Epoch 10 ++++++++++++++++++++\n",
      "Training CE Loss: 5.77608\n",
      "one epoch train: 0.10380816459655762\n",
      "++++++++++++++++++++ Valid Epoch 10 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0, 'NDCG@20': tensor(0., device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n",
      "one epoch val: 0.021220684051513672\n",
      "train_time: 6.5849854946136475\n",
      "val_time: 0.29761815071105957\n",
      "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
      "{'Recall@20': 0.014084506779909134, 'NDCG@20': tensor(0.0061, device='cuda:0'), 'Recall@5': 0.014084506779909134, 'NDCG@5': tensor(0.0061, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(\"pretrain\")\n",
    "model = get_model(args)\n",
    "SeqTrain(args['epochs'], model, train_loader, val_loader, writer, args) #, user_noclick\n",
    "writer.close()\n",
    "\n",
    "if args['eval']:\n",
    "    model = get_model(args)\n",
    "    best_weight = torch.load(os.path.join(args['save_path'],\n",
    "                                              '{}_{}_seed{}_is_pretrain_{}_best_model_lr{}_wd{}_block{}_hd{}_emb{}.pth'.format(\n",
    "                                                  args['task_name'], args['model_name'], args['seed'], \n",
    "                                                  args['is_pretrain'], args['lr'], args['weight_decay'], \n",
    "                                                  args['block_num'], args['hidden_size'], args['embedding_size'])))\n",
    "    \n",
    "    model.load_state_dict(best_weight)\n",
    "    model = model.to(args['device'])\n",
    "    metrics = Sequence_full_Validate(0, model, test_loader, writer, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2ec17f",
   "metadata": {},
   "source": [
    "## Results\n",
    "### we can summarize the evaluation results on Testing as:\n",
    "\n",
    "#### Recall@20 : 0.014084506779909134\n",
    "\n",
    "#### NDCG@20 : 0.0061\n",
    "\n",
    "#### Recall@5 : 0.014084506779909134\n",
    "\n",
    "#### NDCG@5 : 0.0061"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d4279c",
   "metadata": {},
   "source": [
    "## Trial 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc5b09b",
   "metadata": {},
   "source": [
    "- We will train this trial with (learning rate = 0.01, epochs = 5 and batch_size = 50).\n",
    "- We will train the model with a sample of sbr_data_1M (1000000 samples), as it crashes a larger number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ddb259c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T07:53:34.452295Z",
     "iopub.status.busy": "2023-07-25T07:53:34.451572Z",
     "iopub.status.idle": "2023-07-25T07:53:37.851428Z",
     "shell.execute_reply": "2023-07-25T07:53:37.850481Z",
     "shell.execute_reply.started": "2023-07-25T07:53:34.452259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============cold_start=============\n",
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting user ids and item ids in DataFrame ==========\n",
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting item ids in DataFrame ==========\n",
      "+++user_history+++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 38/38 [00:00<00:00, 39697.02it/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"./checkpoint2\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "args = {\n",
    "    'seed': 0,\n",
    "    'task_name': '',\n",
    "    'task_num': 4,\n",
    "    'dataset_path': '',\n",
    "    'pretrain_path': '',\n",
    "    'source_path': '',\n",
    "    'target_path': '',\n",
    "    'train_batch_size': 50,\n",
    "    'val_batch_size': 50,\n",
    "    'test_batch_size': 50,\n",
    "    'sample': 'random',\n",
    "    'negsample_savefolder': './data/neg_data/',\n",
    "    'negsample_size': 99,\n",
    "    'max_len': 20,\n",
    "    'item_min': 10,\n",
    "    'save_path': checkpoint_dir,\n",
    "    'task': -1,\n",
    "    'valid_rate': 100,\n",
    "    'model_name': 'Peter4Coldstart',\n",
    "    'epochs': 5,\n",
    "    're_epochs': 20,\n",
    "    'lr': 0.01,\n",
    "    'device': 'cuda',\n",
    "    'is_parallel': False,\n",
    "    'local_rank': None,\n",
    "    'num_gpu': 1,\n",
    "    'weight_decay': 0.0,\n",
    "    'decay_step': 5,\n",
    "    'gamma': 0.5,\n",
    "    'num_users': 1,\n",
    "    'num_items': 1,\n",
    "    'num_embedding': 1,\n",
    "    'num_labels': 1,\n",
    "    'k': 20,\n",
    "    'metric_ks': [5, 20],\n",
    "    'best_metric': 'NDCG@10',\n",
    "    'hidden_size': 128,\n",
    "    'block_num': 2,\n",
    "    'num_groups': 4,\n",
    "    'num_heads': 4,\n",
    "    'dropout': 0.3,\n",
    "    'bert_mask_prob': 0.3,\n",
    "    'factor_num': 128,\n",
    "    'embedding_size': 128,\n",
    "    'dilations': [1, 4],\n",
    "    'kernel_size': 3,\n",
    "    'is_mp': False,\n",
    "    'pad_token': 0,\n",
    "    'temp': 7,\n",
    "    'l2_emb': 0.0,\n",
    "    'mtl_task_num': 1,\n",
    "    'test_method': 'ufo',\n",
    "    'val_method': 'ufo',\n",
    "    'test_size': 0.1,\n",
    "    'val_size': 0.1111,\n",
    "    'cand_num': 100,\n",
    "    'sample_method': 'high-pop',\n",
    "    'sample_ratio': 0.3,\n",
    "    'num_ng': 4,\n",
    "    'loss_type': 'BPR',\n",
    "    'init_method': 'default',\n",
    "    'optimizer': 'default',\n",
    "    'early_stop': True,\n",
    "    'reg_1': 0.0,\n",
    "    'reg_2': 0.0,\n",
    "    'context_window': 2,\n",
    "    'rho': 0.5,\n",
    "    'node_dropout': 0.1,\n",
    "    'mess_dropout': 0.1,\n",
    "    'hidden_size_list': [128, 128],\n",
    "    'latent_dim': 128,\n",
    "    'anneal_cap': 0.2,\n",
    "    'total_anneal_steps': 1000,\n",
    "    'kd': False,\n",
    "    'alpha': 0.4,\n",
    "    'add_num_times': 2,\n",
    "    'is_pretrain': 1,\n",
    "    'user_profile': 'gender',\n",
    "    'prun_rate': 0,\n",
    "    'll_max_itemnum': 0,\n",
    "    'lifelong_eval': True,\n",
    "    'task1_out': 0,\n",
    "    'task2_out': 0,\n",
    "    'task3_out': 0,\n",
    "    'task4_out': 0,\n",
    "    'eval': True,\n",
    "    'ch':True,\n",
    "}\n",
    "if args['is_parallel']:\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    torch.cuda.set_device(args['local_rank'])\n",
    "device = torch.device(args['device'])\n",
    "set_seed(args['seed'])\n",
    "writer = SummaryWriter()\n",
    "print('=============cold_start=============')\n",
    "args['source_path'] = '/kaggle/input/tenrecdatasets/sbr_data_1M_sampled.csv'\n",
    "args['target_path'] = '/kaggle/input/tenrecdata/cold_data_0.7.csv'\n",
    "train_loader, val_loader, test_loader = get_data(args) #, user_noclick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de810d46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T07:53:37.855631Z",
     "iopub.status.busy": "2023-07-25T07:53:37.852746Z",
     "iopub.status.idle": "2023-07-25T07:53:38.905519Z",
     "shell.execute_reply": "2023-07-25T07:53:38.904507Z",
     "shell.execute_reply.started": "2023-07-25T07:53:37.855603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrain\n",
      "++++++++++++++++++++ Train Epoch 1 ++++++++++++++++++++\n",
      "Training CE Loss: 10.14783\n",
      "one epoch train: 0.16315174102783203\n",
      "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
      "{'Recall@20': 0.029999999329447746, 'NDCG@20': tensor(0.0153, device='cuda:0'), 'Recall@5': 0.009999999776482582, 'NDCG@5': tensor(0.0100, device='cuda:0')}\n",
      "one epoch val: 0.03486299514770508\n",
      "++++++++++++++++++++ Train Epoch 2 ++++++++++++++++++++\n",
      "Training CE Loss: 6.86433\n",
      "one epoch train: 0.15017223358154297\n",
      "++++++++++++++++++++ Valid Epoch 2 ++++++++++++++++++++\n",
      "{'Recall@20': 0.009999999776482582, 'NDCG@20': tensor(0.0024, device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n",
      "one epoch val: 0.02913689613342285\n",
      "++++++++++++++++++++ Train Epoch 3 ++++++++++++++++++++\n",
      "Training CE Loss: 6.26988\n",
      "one epoch train: 0.17031502723693848\n",
      "++++++++++++++++++++ Valid Epoch 3 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0, 'NDCG@20': tensor(0., device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n",
      "one epoch val: 0.04625988006591797\n",
      "++++++++++++++++++++ Train Epoch 4 ++++++++++++++++++++\n",
      "Training CE Loss: 5.86519\n",
      "one epoch train: 0.1454756259918213\n",
      "++++++++++++++++++++ Valid Epoch 4 ++++++++++++++++++++\n",
      "{'Recall@20': 0.009999999776482582, 'NDCG@20': tensor(0.0028, device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n",
      "one epoch val: 0.025452136993408203\n",
      "++++++++++++++++++++ Train Epoch 5 ++++++++++++++++++++\n",
      "Training CE Loss: 5.40198\n",
      "one epoch train: 0.14739251136779785\n",
      "++++++++++++++++++++ Valid Epoch 5 ++++++++++++++++++++\n",
      "{'Recall@20': 0.019999999552965164, 'NDCG@20': tensor(0.0071, device='cuda:0'), 'Recall@5': 0.009999999776482582, 'NDCG@5': tensor(0.0043, device='cuda:0')}\n",
      "one epoch val: 0.025441884994506836\n",
      "train_time: 0.7765071392059326\n",
      "val_time: 0.16115379333496094\n",
      "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
      "{'Recall@20': 0.009999999776482582, 'NDCG@20': tensor(0.0036, device='cuda:0'), 'Recall@5': 0.0, 'NDCG@5': tensor(0., device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(\"pretrain\")\n",
    "model = get_model(args)\n",
    "SeqTrain(args['epochs'], model, train_loader, val_loader, writer, args) #, user_noclick\n",
    "writer.close()\n",
    "\n",
    "if args['eval']:\n",
    "    model = get_model(args)\n",
    "    best_weight = torch.load(os.path.join(args['save_path'],\n",
    "                                              '{}_{}_seed{}_is_pretrain_{}_best_model_lr{}_wd{}_block{}_hd{}_emb{}.pth'.format(\n",
    "                                                  args['task_name'], args['model_name'], args['seed'], \n",
    "                                                  args['is_pretrain'], args['lr'], args['weight_decay'], \n",
    "                                                  args['block_num'], args['hidden_size'], args['embedding_size'])))\n",
    "    \n",
    "    model.load_state_dict(best_weight)\n",
    "    model = model.to(args['device'])\n",
    "    metrics = Sequence_full_Validate(0, model, test_loader, writer, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02178001",
   "metadata": {},
   "source": [
    "## Results\n",
    "### we can summarize the evaluation results on Testing as:\n",
    "\n",
    "#### Recall@20 : 0.009999999776482582\n",
    "\n",
    "#### NDCG@20 : 0.0036\n",
    "\n",
    "#### Recall@5 : 0.0\n",
    "\n",
    "#### NDCG@5 : 0.\n",
    "- As we can notice the performance of privous tunning is better than this one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
