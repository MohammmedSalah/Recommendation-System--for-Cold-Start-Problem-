{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32bb2e98",
   "metadata": {},
   "source": [
    "# Bert4ColdStart using cold_data.csv dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d0c4b7",
   "metadata": {},
   "source": [
    "# Import needed Libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "685a317b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 12:53:05.843364: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-23 12:53:06.613622: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-23 12:53:06.613701: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-23 12:53:06.613709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "\n",
    "import json\n",
    "import joblib\n",
    "import pickle\n",
    "import torch.utils.data as data_utils\n",
    "import scipy.sparse as sp\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "tqdm.pandas()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Set to the index of the GPU you want to use\n",
    "torch.cuda.set_per_process_memory_fraction(0.8)  # Limit memory usage to 80%\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "#Ignore warning\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875391e",
   "metadata": {},
   "source": [
    "### Checks whether a GPU is available for use with PyTorch or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6c060e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8e323d",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "## here we are setting all the needed setting to make any changes in the project from one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ae01765",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'sbr_sample_size':10000000, #the selected sample size from SBR file\n",
    "    'seed': 0,\n",
    "    'task_name': '',\n",
    "    'task_num': 4,\n",
    "    'dataset_path': '',\n",
    "    'pretrain_path': '',\n",
    "    'source_path': 'sbr_data_1M.csv',\n",
    "    'target_path': 'cold_data.csv',\n",
    "    'train_batch_size': 1024,\n",
    "    'val_batch_size': 1024,\n",
    "    'test_batch_size': 1024,\n",
    "    'sample': 'random',\n",
    "    'negsample_savefolder': './data/neg_data/',\n",
    "    'negsample_size': 99,\n",
    "    'max_len': 20,\n",
    "    'item_min': 5,\n",
    "    'save_path': './checkpoint/',\n",
    "    'task': -1,\n",
    "    'valid_rate': 100,\n",
    "    'model_name': 'BERT_ColdstartModel',\n",
    "    'epochs': 20,\n",
    "    're_epochs': 20,\n",
    "    'lr': 0.0005,\n",
    "    'device': 'cuda',\n",
    "    'is_parallel': False,\n",
    "    'local_rank': None,\n",
    "    'num_gpu': 1,\n",
    "    'weight_decay': 0.0,\n",
    "    'decay_step': 5,\n",
    "    'gamma': 0.5,\n",
    "    'num_users': 1,\n",
    "    'num_items': 1,\n",
    "    'num_embedding': 1,\n",
    "    'num_labels': 1,\n",
    "    'k': 20,\n",
    "    'metric_ks': [5, 20],\n",
    "    'best_metric': 'NDCG@10',\n",
    "    'hidden_size': 128,\n",
    "    'block_num': 2,\n",
    "    'num_groups': 4,\n",
    "    'num_heads': 4,\n",
    "    'dropout': 0.3,\n",
    "    'bert_mask_prob': 0.3,\n",
    "    'factor_num': 128,\n",
    "    'embedding_size': 128,\n",
    "    'dilations': [1, 4],\n",
    "    'kernel_size': 3,\n",
    "    'is_mp': False,\n",
    "    'pad_token': 0,\n",
    "    'temp': 7,\n",
    "    'l2_emb': 0.0,\n",
    "    'mtl_task_num': 1,\n",
    "    'test_method': 'ufo',\n",
    "    'val_method': 'ufo',\n",
    "    'test_size': 0.1,\n",
    "    'val_size': 0.1111,\n",
    "    'cand_num': 100,\n",
    "    'sample_method': 'high-pop',\n",
    "    'sample_ratio': 0.3,\n",
    "    'num_ng': 4,\n",
    "    'loss_type': 'BPR',\n",
    "    'init_method': 'default',\n",
    "    'optimizer': 'default',\n",
    "    'early_stop': True,\n",
    "    'reg_1': 0.0,\n",
    "    'reg_2': 0.0,\n",
    "    'context_window': 2,\n",
    "    'rho': 0.5,\n",
    "    'node_dropout': 0.1,\n",
    "    'mess_dropout': 0.1,\n",
    "    'hidden_size_list': [128, 128],\n",
    "    'latent_dim': 128,\n",
    "    'anneal_cap': 0.2,\n",
    "    'total_anneal_steps': 1000,\n",
    "    'kd': False,\n",
    "    'alpha': 0.4,\n",
    "    'add_num_times': 2,\n",
    "    'is_pretrain': 1,\n",
    "    'user_profile': 'gender',\n",
    "    'prun_rate': 0,\n",
    "    'll_max_itemnum': 0,\n",
    "    'lifelong_eval': True,\n",
    "    'task1_out': 0,\n",
    "    'task2_out': 0,\n",
    "    'task3_out': 0,\n",
    "    'task4_out': 0,\n",
    "    'eval': True,\n",
    "    'ch':True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce5fb1",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f44e7b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select sample from the SBR file sbr_data_1M.csv as it's to large to be computed\n",
    "\n",
    "def generate_sbr_sample():\n",
    "    df = pd.read_csv(\"sbr_data_1M.csv\")\n",
    "    sampled_df = df.sample(args['sbr_sample_size'])\n",
    "    print('sample size : ', sampled_df.count())\n",
    "    file_name = \"sbr_sample_\"+str(args['sbr_sample_size'])+\".csv\"\n",
    "    args['source_path'] = file_name #update the source path with the generated sampled SBR\n",
    "    sampled_df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7c8833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Item ID Resetter with Label Encoding\n",
    "class item_reset_df(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"=\" * 10, \"Initialize Reset DataFrame Object\", \"=\" * 10)\n",
    "        self.item_enc = LabelEncoder()\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "        print(\"=\" * 10, \"Resetting item ids in DataFrame\", \"=\" * 10)\n",
    "        df['item_id'] = self.item_enc.fit_transform(df['item_id']) + 1\n",
    "        return df\n",
    "\n",
    "    def inverse_transform(self, df):\n",
    "        df['item_id'] = self.item_enc.inverse_transform(df['item_id']) - 1\n",
    "        return df\n",
    "    \n",
    "#Construct training data\n",
    "def construct_data(args, item_min):\n",
    "    path1 = args['target_path']\n",
    "    path2 = args['source_path']\n",
    "    df1 = pd.read_csv(path1, usecols=['user_id', 'item_id', 'click'])\n",
    "    df1 = df1[df1.click.isin([1])]\n",
    "    df2 = pd.read_csv(path2, usecols=['user_id', 'item_id', 'click'])\n",
    "    df2 = df2[df2.click.isin([1])]\n",
    "    user_counts = df2.groupby('user_id').size()\n",
    "    user_subset = np.in1d(df2.user_id, user_counts[user_counts >= item_min].index)\n",
    "    df2 = df2[user_subset].reset_index(drop=True)\n",
    "\n",
    "    assert (df2.groupby('user_id').size() < item_min).sum() == 0\n",
    "    s_item_count = len(set(df2['item_id']))\n",
    "    reset_ob = cold_reset_df()\n",
    "    df2, df1 = reset_ob.fit_transform(df2, df1)\n",
    "    user1 = set(df1.user_id.values.tolist())\n",
    "    user2 = set(df2.user_id.values.tolist())\n",
    "    user = user1 & user2\n",
    "    df1 = df1[df1.user_id.isin(list(user))]\n",
    "    df2 = df2[df2.user_id.isin(list(user))]\n",
    "    new_data1 = []\n",
    "    new_data2 = []\n",
    "    for u in user:\n",
    "        tmp_data2 = df2[df2.user_id == u][:-3].values.tolist()\n",
    "        tmp_data1 = df1[df1.user_id == u].values.tolist()\n",
    "        new_data1.extend(tmp_data1)\n",
    "        new_data2.extend(tmp_data2)\n",
    "        \n",
    "    new_data1 = pd.DataFrame(new_data1, columns=df1.columns)\n",
    "    new_data2 = pd.DataFrame(new_data2, columns=df2.columns)\n",
    "    user_count = len(set(new_data1.user_id.values.tolist()))\n",
    "    reset_item = item_reset_df()\n",
    "    new_data1 = reset_item.fit_transform(new_data1)\n",
    "    t_item_count = len(set(new_data1['item_id']))\n",
    "    return new_data1, new_data2, user_count, t_item_count, s_item_count\n",
    "\n",
    "#Construct cold-start dataset\n",
    "def colddataset(item_min, args, path=None):\n",
    "    target_data, source_data, user_count, t_item_count, s_item_count = construct_data(args, item_min)\n",
    "    print(\"+++user_history+++\")\n",
    "    user_history = source_data.groupby('user_id').item_id.apply(list).to_dict()\n",
    "    target = target_data.groupby('user_id').item_id.apply(list).to_dict()\n",
    "\n",
    "    examples = []\n",
    "    for u, t_list in tqdm(target.items()):\n",
    "        for t in t_list:\n",
    "            e_list = [user_history[u] + [0], t]\n",
    "            examples.append(e_list)\n",
    "    examples = pd.DataFrame(examples, columns=['source', 'target'])\n",
    "    return examples, user_count, s_item_count, t_item_count\n",
    "\n",
    "\n",
    "class cold_reset_df(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"=\" * 10, \"Initialize Reset DataFrame Object\", \"=\" * 10)\n",
    "        self.item_enc1 = LabelEncoder()\n",
    "        self.item_enc2 = LabelEncoder()\n",
    "        self.user_enc = LabelEncoder()\n",
    "\n",
    "    def fit_transform(self, df1, df2):\n",
    "        print(\"=\" * 10, \"Resetting user ids and item ids in DataFrame\", \"=\" * 10)\n",
    "        df = pd.concat([df1['user_id'], df2['user_id']])\n",
    "        df = self.user_enc.fit_transform(df) + 1\n",
    "        df1['item_id'] = self.item_enc1.fit_transform(df1['item_id']) + 1\n",
    "        df1['user_id'] = df[:len(df1)]\n",
    "        df2['item_id'] = self.item_enc2.fit_transform(df2['item_id']) + 1\n",
    "        df2['user_id'] = df[len(df1):]\n",
    "        return df1, df2\n",
    "\n",
    "    def inverse_transform(self, df):\n",
    "        df['item_id'] = self.item_enc.inverse_transform(df['item_id']) - 1\n",
    "        df['user_id'] = self.user_enc.inverse_transform(df['user_id']) - 1\n",
    "        return df\n",
    "\n",
    "class ColdDataset(data_utils.Dataset):\n",
    "    def __init__(self, x, y, max_len, mask_token):\n",
    "        self.seqs = x\n",
    "        self.targets = y\n",
    "        self.max_len = max_len\n",
    "        self.mask_token = mask_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = self.seqs[index]\n",
    "        target = self.targets[index]\n",
    "        seq = seq[-self.max_len:]\n",
    "        seq_len = len(seq)\n",
    "        seq_mask_len = self.max_len - seq_len\n",
    "        seq = [0] * seq_mask_len + seq\n",
    "        return torch.LongTensor(seq), torch.LongTensor([target])\n",
    "\n",
    "class ColdEvalDataset(data_utils.Dataset):\n",
    "    def __init__(self, x, y, max_len, mask_token, num_item):\n",
    "        self.seqs = x\n",
    "        self.targets = y\n",
    "        self.max_len = max_len\n",
    "        self.mask_token = mask_token\n",
    "        self.num_item = num_item + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = self.seqs[index]\n",
    "        target = self.targets[index]\n",
    "        labels = [0] * self.num_item\n",
    "        labels[target] = 1\n",
    "        seq = seq[-self.max_len:]\n",
    "        seq_len = len(seq)\n",
    "        seq_mask_len = self.max_len - seq_len\n",
    "        seq = [self.mask_token] * seq_mask_len + seq\n",
    "        return torch.LongTensor(seq), torch.LongTensor(labels)\n",
    "\n",
    "# create and return a PyTorch data loader for a training dataset\n",
    "def get_train_loader(dataset, args):\n",
    "    if args['is_parallel']:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['train_batch_size'], sampler=DistributedSampler(dataset))\n",
    "    else:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['train_batch_size'], shuffle=True, pin_memory=True)\n",
    "    return dataloader\n",
    "\n",
    "# create and return a PyTorch data loader for a validation dataset\n",
    "def get_val_loader(dataset, args):\n",
    "    if args['is_parallel']:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['val_batch_size'], sampler=DistributedSampler(dataset))\n",
    "    else:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['val_batch_size'], shuffle=False, pin_memory=True)\n",
    "    return dataloader\n",
    "\n",
    "# create and return a PyTorch data loader for a testing dataset\n",
    "def get_test_loader(dataset, args):\n",
    "    if args['is_parallel']:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['test_batch_size'], sampler=DistributedSampler(dataset))\n",
    "    else:\n",
    "        dataloader = data_utils.DataLoader(dataset, batch_size=args['test_batch_size'], shuffle=False, pin_memory=True)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d570ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation and Splitting Function for Cold Start Recommendation System\n",
    "def get_data(args):\n",
    "    path = args['dataset_path']\n",
    "    rng = random.Random(args['seed'])\n",
    "    data, user_count, vocab_size, item_count = colddataset(args['item_min'], args)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data.source.values.tolist(),\n",
    "                                                            data.target.values.tolist(),\n",
    "                                                            test_size=0.2, random_state=args['seed'])\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=args['seed'])\n",
    "    # not hot and cold#####\n",
    "\n",
    "    args['num_users'] = user_count\n",
    "    args['num_items'] = item_count\n",
    "    args['num_embedding'] = vocab_size\n",
    "\n",
    "    train_dataset, valid_dataset = ColdDataset(x_train, y_train, args['max_len'], args['pad_token']), ColdEvalDataset(\n",
    "        x_val, y_val, args['max_len'], args['pad_token'], args['num_items'])\n",
    "    test_dataset = ColdEvalDataset(x_test, y_test, args['max_len'], args['pad_token'], args['num_items'])\n",
    "    train_dataloader = get_train_loader(train_dataset, args)\n",
    "    valid_dataloader = get_val_loader(valid_dataset, args)\n",
    "    test_dataloader = get_test_loader(test_dataset, args)\n",
    "    return train_dataloader, valid_dataloader, test_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603e87dc",
   "metadata": {},
   "source": [
    "# Bert4ColdStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c13b4b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "                 / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper Section 3.4, last paragraph notice that BERT used the GELU instead of RELU\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        return self.pe.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size=512):\n",
    "        super().__init__(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Embedding which is consisted with under features\n",
    "        1. TokenEmbedding : normal embedding matrix\n",
    "        2. PositionalEmbedding : adding positional information using sin, cos\n",
    "        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n",
    "\n",
    "        sum of all these features are output of BERTEmbedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, max_len, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: total vocab size\n",
    "        :param embed_size: embedding size of token embedding\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(max_len=max_len, d_model=embed_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        x = self.token(sequence) + self.position(sequence)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional Encoder = Transformer (self-attention)\n",
    "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        \"\"\"\n",
    "        :param hidden: hidden size of transformer\n",
    "        :param attn_heads: head sizes of multi-head attention\n",
    "        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class bottle_net(nn.Module):\n",
    "    def __init__(self, hidden):\n",
    "        super(bottle_net, self).__init__()\n",
    "        self.hidden = hidden\n",
    "        self.hidden_size = int(hidden / 4)\n",
    "        self.linear1 = nn.Linear(self.hidden, self.hidden_size)\n",
    "        self.linear2 = nn.Linear(self.hidden_size, self.hidden)\n",
    "        self.ln = nn.LayerNorm(self.hidden, eps=1e-8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = F.leaky_relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.ln(out)\n",
    "        return out\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        max_len =   args['max_len']\n",
    "        n_layers =  args['block_num']\n",
    "        heads = args['num_heads']\n",
    "        vocab_size = args['num_embedding'] + 1\n",
    "        hidden = args['hidden_size'] \n",
    "        self.hidden = hidden\n",
    "        self.is_mp = args['is_mp']\n",
    "        dropout = args['dropout']\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=self.hidden, max_len=max_len, dropout=dropout)\n",
    "\n",
    "        transformer_blocks = []\n",
    "        for _ in range(n_layers):\n",
    "            transformer_blocks.append(TransformerBlock(hidden, heads, hidden * 4, dropout))\n",
    "            if self.is_mp:\n",
    "                transformer_blocks.append(bottle_net(self.hidden))\n",
    "        self.transformer_blocks = nn.ModuleList(transformer_blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x)\n",
    "        # running over multiple transformer blocks\n",
    "        for i, block in enumerate(self.transformer_blocks):\n",
    "            mp_input = x\n",
    "            if i % 2 == 0:\n",
    "                x = block.forward(x, mask)\n",
    "            else:\n",
    "                if self.is_mp:\n",
    "                    mp_out = block(mp_input)\n",
    "                    x = mp_out + x\n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        pass\n",
    "\n",
    "class BERT_ColdstartModel(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.bert = BERT(args)\n",
    "        self.num_items = args['num_items']\n",
    "        self.out = nn.Linear(self.bert.hidden*args['max_len'], args['num_items'] + 1)#update\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bert(x)\n",
    "        \n",
    "        return self.out(x.view(-1,self.bert.hidden*args['max_len']))#update\n",
    "\n",
    "    def predict(self, x, item):\n",
    "        x = self.bert(x)\n",
    "        item_emb = self.bert.embedding.token(item)\n",
    "        logits = x.matmul(item_emb.transpose(1, 2))\n",
    "        print('shape logits')\n",
    "#         print(logits.shape)\n",
    "        logits = logits.mean(1)\n",
    "#         print(logits.shape)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68e91ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generic function get the configured model in args\n",
    "def get_model(args, linear_feature_columns=None, dnn_feature_columns=None, history_feature_list=None):\n",
    "    if args['model_name'] == 'BERT_ColdstartModel':\n",
    "        return BERT_ColdstartModel(args)\n",
    "    elif args['model_name'] == 'Peter4Coldstart':\n",
    "        return Peter4Coldstart(args)\n",
    "    \n",
    "    # elif name == 'vae':\n",
    "    #     return VAECF(args)\n",
    "    # elif name == 'item2vec':\n",
    "    #     return Item2Vec(args)\n",
    "    else:\n",
    "        raise ValueError('unknown model name: ' +  args['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "079faca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, re=True):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    if re:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    else:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd48853f",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "711bc78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform validation on a recommendation model.\n",
    "def Sequence_full_Validate(epoch, model, dataloader, writer, args, test=False):\n",
    "    print(\"+\" * 20, \"Valid Epoch {}\".format(epoch + 1), \"+\" * 20)\n",
    "    model.eval()\n",
    "    avg_metrics = {}\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        tqdm_dataloader = dataloader\n",
    "        for data in tqdm_dataloader:\n",
    "            data = [x.to(args['device']) for x in data]\n",
    "            seqs, labels = data\n",
    "            if test:\n",
    "                scores = model.predict(seqs)\n",
    "            else:\n",
    "                scores = model(seqs)\n",
    "#             scores = scores.mean(1) #update\n",
    "#             print('score')\n",
    "#             print(scores.shape)\n",
    "#             print(labels.shape)\n",
    "            metrics = recalls_and_ndcgs_for_ks(scores, labels, args['metric_ks'], args)\n",
    "            i += 1\n",
    "            for key, value in metrics.items():\n",
    "                if key not in avg_metrics:\n",
    "                    avg_metrics[key] = value\n",
    "                else:\n",
    "                    avg_metrics[key] += value\n",
    "    for key, value in avg_metrics.items():\n",
    "        avg_metrics[key] = value / i\n",
    "    print(avg_metrics)\n",
    "    for k in sorted(args['metric_ks'], reverse=True):\n",
    "        writer.add_scalar('Train/NDCG@{}'.format(k), avg_metrics['NDCG@%d' % k], epoch)\n",
    "    return avg_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c508eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the recall and NDCG@ks metrics \n",
    "def recalls_and_ndcgs_for_ks(scores, labels, ks, args):\n",
    "    metrics = {}\n",
    "\n",
    "    answer_count = labels.sum(1)\n",
    "    answer_count_float = answer_count.float()\n",
    "    labels_float = labels.float()\n",
    "#     print(scores.shape)\n",
    "    rank = (-scores).argsort(dim=0)\n",
    "    cut = rank\n",
    "    for k in sorted(ks, reverse=True):\n",
    "#        print(cut.shape)\n",
    "       cut = cut[:, :k]\n",
    "#        print(labels_float.shape)\n",
    "       hits = torch.gather(labels_float,1,cut)\n",
    "#         labels_float.gather( cut,0)\n",
    "       metrics['Recall@%d' % k] = (hits.sum(1) / answer_count_float).mean().item()\n",
    "\n",
    "       position = torch.arange(2, 2+k)\n",
    "       weights = 1 / torch.log2(position.float()).to(args['device'])\n",
    "#        print(weights.shape)\n",
    "#        print(hits.shape) \n",
    "       dcg = (hits * weights).sum(1)\n",
    "       idcg = torch.Tensor([weights[:min(n, k)].sum() for n in answer_count]).to(args['device'])\n",
    "       ndcg = (dcg / idcg).mean()\n",
    "       metrics['NDCG@%d' % k] = ndcg\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8558ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a sequence-based recommendation model for one epoc\n",
    "def SequenceTrainer(epoch, model, dataloader, optimizer, writer, args): #schedular,\n",
    "    print(\"+\" * 20, \"Train Epoch {}\".format(epoch + 1), \"+\" * 20)\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        data = [x.to(args['device']) for x in data]\n",
    "        seqs, labels = data\n",
    "#         print('data shape',labels.shape)\n",
    "        logits = model(seqs) # B x T x V\n",
    "#         print('pred shape',logits.shape)\n",
    "        if 'cold' in args['task_name'] or ('life_long' in args['task_name'] and args['task'] != 0):\n",
    "            logits = logits.mean(1)\n",
    "            labels = labels.view(-1)\n",
    "        else:\n",
    "            logits = logits.view(-1, logits.size(-1)) # (B*T) x V\n",
    "            labels = labels.view(-1)  # B*T\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.detach().cpu().item()\n",
    "    writer.add_scalar('Train/loss', running_loss / len(dataloader), epoch)\n",
    "    print(\"Training CE Loss: {:.5f}\".format(running_loss / len(dataloader)))\n",
    "    return optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "870c52a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SeqTrain(epochs, model, train_loader, val_loader, writer, args):\n",
    "    if args['is_pretrain'] == 0:\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                     lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "    model = model.to(args['device'])\n",
    "    if args['is_parallel']:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model,  find_unused_parameters=True,device_ids=[args['local_rank']], output_device=[args['local_rank']])\n",
    "    best_metric = 0\n",
    "    all_time = 0\n",
    "    val_all_time = 0\n",
    "    for epoch in range(epochs):\n",
    "        since = time.time()\n",
    "        optimizer = SequenceTrainer(epoch, model, train_loader, optimizer, writer, args)\n",
    "        tmp = time.time() - since\n",
    "        print('one epoch train:', tmp)\n",
    "        all_time += tmp\n",
    "        val_since = time.time()\n",
    "        metrics = Sequence_full_Validate(epoch, model, val_loader, writer, args)\n",
    "        val_tmp = time.time() - val_since\n",
    "        print('one epoch val:', val_tmp)\n",
    "        val_all_time += val_tmp\n",
    "        if args['is_pretrain'] == 0 and 'acc' in args['task_name']:\n",
    "            if metrics['NDCG@20'] >= 0.0193:\n",
    "                break\n",
    "        i = 1\n",
    "        current_metric = metrics['NDCG@5']\n",
    "        if best_metric <= current_metric:\n",
    "            best_metric = current_metric\n",
    "            best_model = deepcopy(model)\n",
    "            state_dict = model.state_dict()\n",
    "            if 'life' in args['task_name']:\n",
    "                torch.save(state_dict, os.path.join(args['save_path'],\n",
    "                                                         '{}_{}_seed{}_task_{}_best_model.pth'.format('sequence',\n",
    "                                                                                                      args['model_name'],\n",
    "                                                                                                      args['seed'],\n",
    "                                                                                                      args['task'])))\n",
    "            else:\n",
    "                torch.save(state_dict, os.path.join(args['save_path'], '{}_{}_seed{}_is_pretrain_{}_best_model_lr{}_wd{}_block{}_hd{}_emb{}.pth'.format(args['task_name'], args['model_name'], args['seed'], args['is_pretrain'],\n",
    "                                                                                                                              args['lr'], args['weight_decay'], args['block_num'], args['hidden_size'], args['embedding_size'])))\n",
    "        else:\n",
    "            i += 1\n",
    "            if i == 10:\n",
    "                print('early stop!')\n",
    "                break\n",
    "    print('train_time:', all_time)\n",
    "    print('val_time:', val_all_time)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ffde6a",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46c5a6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============cold_start=============\n",
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting user ids and item ids in DataFrame ==========\n",
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting item ids in DataFrame ==========\n",
      "+++user_history+++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39583/39583 [00:00<00:00, 41003.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate sample from SBR file\n"
     ]
    }
   ],
   "source": [
    "if args['is_parallel']:\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    torch.cuda.set_device(args['local_rank'])\n",
    "device = torch.device(args['device'])\n",
    "set_seed(args['seed'])\n",
    "writer = SummaryWriter()\n",
    "print('=============cold_start=============')\n",
    "train_loader, val_loader, test_loader = get_data(args) #, user_noclick\n",
    "\n",
    "print('generate sample from SBR file')\n",
    "generate_sbr_sample()\n",
    "##########################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e84fbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrain\n",
      "++++++++++++++++++++ Train Epoch 1 ++++++++++++++++++++\n",
      "Training CE Loss: 10.87988\n",
      "one epoch train: 15.131677389144897\n",
      "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
      "{'Recall@20': 0.00087890625, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.000146484375, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 137.24546194076538\n",
      "++++++++++++++++++++ Train Epoch 2 ++++++++++++++++++++\n",
      "Training CE Loss: 8.68420\n",
      "one epoch train: 15.144231796264648\n",
      "++++++++++++++++++++ Valid Epoch 2 ++++++++++++++++++++\n",
      "{'Recall@20': 0.00078125, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.0001953125, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 133.67519760131836\n",
      "++++++++++++++++++++ Train Epoch 3 ++++++++++++++++++++\n",
      "Training CE Loss: 6.71751\n",
      "one epoch train: 15.138242959976196\n",
      "++++++++++++++++++++ Valid Epoch 3 ++++++++++++++++++++\n",
      "{'Recall@20': 0.000537109375, 'NDCG@20': tensor(0.0002, device='cuda:0'), 'Recall@5': 9.765625e-05, 'NDCG@5': tensor(3.9918e-05, device='cuda:0')}\n",
      "one epoch val: 125.37354946136475\n",
      "++++++++++++++++++++ Train Epoch 4 ++++++++++++++++++++\n",
      "Training CE Loss: 5.57323\n",
      "one epoch train: 14.659057855606079\n",
      "++++++++++++++++++++ Valid Epoch 4 ++++++++++++++++++++\n",
      "{'Recall@20': 0.000634765625, 'NDCG@20': tensor(0.0002, device='cuda:0'), 'Recall@5': 0.000146484375, 'NDCG@5': tensor(9.7656e-05, device='cuda:0')}\n",
      "one epoch val: 132.22924184799194\n",
      "++++++++++++++++++++ Train Epoch 5 ++++++++++++++++++++\n",
      "Training CE Loss: 5.00765\n",
      "one epoch train: 15.823558568954468\n",
      "++++++++++++++++++++ Valid Epoch 5 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0014676131075248122, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.000390625, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 137.69003319740295\n",
      "++++++++++++++++++++ Train Epoch 6 ++++++++++++++++++++\n",
      "Training CE Loss: 4.65192\n",
      "one epoch train: 15.743531227111816\n",
      "++++++++++++++++++++ Valid Epoch 6 ++++++++++++++++++++\n",
      "{'Recall@20': 0.00068359375, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.000244140625, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 134.56682920455933\n",
      "++++++++++++++++++++ Train Epoch 7 ++++++++++++++++++++\n",
      "Training CE Loss: 4.39565\n",
      "one epoch train: 14.806526899337769\n",
      "++++++++++++++++++++ Valid Epoch 7 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0009765625, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.0001953125, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 127.9736156463623\n",
      "++++++++++++++++++++ Train Epoch 8 ++++++++++++++++++++\n",
      "Training CE Loss: 4.21041\n",
      "one epoch train: 15.197021722793579\n",
      "++++++++++++++++++++ Valid Epoch 8 ++++++++++++++++++++\n",
      "{'Recall@20': 0.000830078125, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.000341796875, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 146.10613536834717\n",
      "++++++++++++++++++++ Train Epoch 9 ++++++++++++++++++++\n",
      "Training CE Loss: 4.07196\n",
      "one epoch train: 15.223116159439087\n",
      "++++++++++++++++++++ Valid Epoch 9 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0016168668400496245, 'NDCG@20': tensor(0.0006, device='cuda:0'), 'Recall@5': 0.0003933943575248122, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 137.82650446891785\n",
      "++++++++++++++++++++ Train Epoch 10 ++++++++++++++++++++\n",
      "Training CE Loss: 3.95561\n",
      "one epoch train: 14.844995498657227\n",
      "++++++++++++++++++++ Valid Epoch 10 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0017117537325248122, 'NDCG@20': tensor(0.0007, device='cuda:0'), 'Recall@5': 0.0005398787325248122, 'NDCG@5': tensor(0.0004, device='cuda:0')}\n",
      "one epoch val: 130.27567505836487\n",
      "++++++++++++++++++++ Train Epoch 11 ++++++++++++++++++++\n",
      "Training CE Loss: 3.86691\n",
      "one epoch train: 14.901937007904053\n",
      "++++++++++++++++++++ Valid Epoch 11 ++++++++++++++++++++\n",
      "{'Recall@20': 0.00107421875, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.000146484375, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 134.45081663131714\n",
      "++++++++++++++++++++ Train Epoch 12 ++++++++++++++++++++\n",
      "Training CE Loss: 3.79107\n",
      "one epoch train: 14.796399116516113\n",
      "++++++++++++++++++++ Valid Epoch 12 ++++++++++++++++++++\n",
      "{'Recall@20': 0.000830078125, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.0001953125, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 136.40996408462524\n",
      "++++++++++++++++++++ Train Epoch 13 ++++++++++++++++++++\n",
      "Training CE Loss: 3.72455\n",
      "one epoch train: 15.904345273971558\n",
      "++++++++++++++++++++ Valid Epoch 13 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0009793318575248122, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.000146484375, 'NDCG@5': tensor(6.9857e-05, device='cuda:0')}\n",
      "one epoch val: 143.69334506988525\n",
      "++++++++++++++++++++ Train Epoch 14 ++++++++++++++++++++\n",
      "Training CE Loss: 3.66151\n",
      "one epoch train: 14.6848726272583\n",
      "++++++++++++++++++++ Valid Epoch 14 ++++++++++++++++++++\n",
      "{'Recall@20': 0.001025390625, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.000341796875, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 139.0303511619568\n",
      "++++++++++++++++++++ Train Epoch 15 ++++++++++++++++++++\n",
      "Training CE Loss: 3.61673\n",
      "one epoch train: 15.20221996307373\n",
      "++++++++++++++++++++ Valid Epoch 15 ++++++++++++++++++++\n",
      "{'Recall@20': 0.000927734375, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.000244140625, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 130.90301322937012\n",
      "++++++++++++++++++++ Train Epoch 16 ++++++++++++++++++++\n",
      "Training CE Loss: 3.56804\n",
      "one epoch train: 15.682220935821533\n",
      "++++++++++++++++++++ Valid Epoch 16 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0010281599825248122, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.000146484375, 'NDCG@5': tensor(9.8525e-05, device='cuda:0')}\n",
      "one epoch val: 133.51218843460083\n",
      "++++++++++++++++++++ Train Epoch 17 ++++++++++++++++++++\n",
      "Training CE Loss: 3.52218\n",
      "one epoch train: 14.961508989334106\n",
      "++++++++++++++++++++ Valid Epoch 17 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0013211287325248123, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.0005398787325248122, 'NDCG@5': tensor(0.0003, device='cuda:0')}\n",
      "one epoch val: 140.8405704498291\n",
      "++++++++++++++++++++ Train Epoch 18 ++++++++++++++++++++\n",
      "Training CE Loss: 3.48533\n",
      "one epoch train: 15.321148157119751\n",
      "++++++++++++++++++++ Valid Epoch 18 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0013699568575248123, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.000146484375, 'NDCG@5': tensor(8.2643e-05, device='cuda:0')}\n",
      "one epoch val: 131.97028756141663\n",
      "++++++++++++++++++++ Train Epoch 19 ++++++++++++++++++++\n",
      "Training CE Loss: 3.45073\n",
      "one epoch train: 15.215424060821533\n",
      "++++++++++++++++++++ Valid Epoch 19 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0009765625, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.000244140625, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 139.7987825870514\n",
      "++++++++++++++++++++ Train Epoch 20 ++++++++++++++++++++\n",
      "Training CE Loss: 3.40362\n",
      "one epoch train: 14.691898584365845\n",
      "++++++++++++++++++++ Valid Epoch 20 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0009793318575248122, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 9.765625e-05, 'NDCG@5': tensor(4.8828e-05, device='cuda:0')}\n",
      "one epoch val: 148.9053373336792\n",
      "train_time: 303.0739347934723\n",
      "val_time: 2722.4769003391266\n",
      "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0009765625, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.000341796875, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "def process_training():\n",
    "    if args['is_pretrain'] == 1:\n",
    "        print(\"pretrain\")\n",
    "        model = get_model(args)\n",
    "        SeqTrain(args['epochs'], model, train_loader, val_loader, writer, args) #, user_noclick\n",
    "        writer.close()\n",
    "    elif args['is_pretrain'] == 2:\n",
    "        args['is_mp'] = False\n",
    "        model = get_model(args)\n",
    "        SeqTrain(args['epochs'], model, train_loader, val_loader, writer, args)\n",
    "        writer.close()\n",
    "    else:\n",
    "        print(\"transfer\")\n",
    "        best_weight = torch.load(args['pretrain_path'])\n",
    "        if 'peter' in args['model_name']:\n",
    "            args.is_mp = True\n",
    "            best_weight.pop('final_layer.weight')\n",
    "            best_weight.pop('final_layer.bias')\n",
    "        if 'bert' in args.model_name:\n",
    "            best_weight.pop('out.weight')\n",
    "            best_weight.pop('out.bias')\n",
    "\n",
    "        model = get_model(args)\n",
    "\n",
    "        model_state = model.module.state_dict() if args.is_parallel else model.state_dict()\n",
    "        best_weight = {k: v for k, v in best_weight.items() if k in model_state}\n",
    "        model_state.update(best_weight)\n",
    "        model.load_state_dict(model_state)\n",
    "            # if 'peter' in args.model_name:\n",
    "            #     for name, parm in model.named_parameters():\n",
    "            #         if 'rez' not in name and 'mp' not in name and 'final_layer' not in name:\n",
    "            #             parm.requires_grad = False\n",
    "            # else:\n",
    "            #     for name, parm in model.named_parameters():\n",
    "            #         if 'out' not in name:\n",
    "            #             parm.requires_grad = False\n",
    "        SeqTrain(args.epochs, model, train_loader, val_loader, writer, args)\n",
    "        writer.close()\n",
    "        \n",
    "    if args['eval']:\n",
    "        model = get_model(args)\n",
    "        best_weight = torch.load(os.path.join(args['save_path'],\n",
    "                                                  '{}_{}_seed{}_is_pretrain_{}_best_model_lr{}_wd{}_block{}_hd{}_emb{}.pth'.format(args['task_name'], args['model_name'], args['seed'], args['is_pretrain'],\n",
    "                                                                                                                                  args['lr'], args['weight_decay'], args['block_num'], args['hidden_size'], args['embedding_size'])))\n",
    "        model.load_state_dict(best_weight)\n",
    "        model = model.to(args['device'])\n",
    "        metrics = Sequence_full_Validate(0, model, test_loader, writer, args)\n",
    "\n",
    "        \n",
    "process_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8485d801",
   "metadata": {},
   "source": [
    "## Results\n",
    "### we can summarize the evaluation results as:\n",
    "\n",
    "#### Recall@20 : 0.0009765625\n",
    "\n",
    "#### NDCG@20 : 0.0004\n",
    "\n",
    "#### Recall@5 : 0.000341796875\n",
    "\n",
    "#### NDCG@5 : 0.0002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d82d8e",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning - Trial 1\n",
    "\n",
    "## will perofrm changes in the Hyperparameters and check if the accuracy enhanced or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "835ca943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Hyperparameter Tuning - cold_start =============\n",
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting user ids and item ids in DataFrame ==========\n",
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting item ids in DataFrame ==========\n",
      "+++user_history+++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39583/39583 [00:01<00:00, 28395.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate sample from SBR file\n",
      "pretrain\n",
      "++++++++++++++++++++ Train Epoch 1 ++++++++++++++++++++\n",
      "Training CE Loss: 10.79594\n",
      "one epoch train: 15.305969715118408\n",
      "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
      "{'Recall@20': 0.000830078125, 'NDCG@20': tensor(0.0002, device='cuda:0'), 'Recall@5': 9.765625e-05, 'NDCG@5': tensor(5.5221e-05, device='cuda:0')}\n",
      "one epoch val: 142.0566484928131\n",
      "++++++++++++++++++++ Train Epoch 2 ++++++++++++++++++++\n",
      "Training CE Loss: 9.19213\n",
      "one epoch train: 14.93803334236145\n",
      "++++++++++++++++++++ Valid Epoch 2 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0014676131075248122, 'NDCG@20': tensor(0.0006, device='cuda:0'), 'Recall@5': 0.0003933943575248122, 'NDCG@5': tensor(0.0003, device='cuda:0')}\n",
      "one epoch val: 135.61878061294556\n",
      "++++++++++++++++++++ Train Epoch 3 ++++++++++++++++++++\n",
      "Training CE Loss: 7.78859\n",
      "one epoch train: 14.91055679321289\n",
      "++++++++++++++++++++ Valid Epoch 3 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0014187849825248121, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.0003933943575248122, 'NDCG@5': tensor(0.0003, device='cuda:0')}\n",
      "one epoch val: 142.26126265525818\n",
      "++++++++++++++++++++ Train Epoch 4 ++++++++++++++++++++\n",
      "Training CE Loss: 6.33711\n",
      "one epoch train: 15.791029930114746\n",
      "++++++++++++++++++++ Valid Epoch 4 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0011746443575248121, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 9.765625e-05, 'NDCG@5': tensor(7.9635e-05, device='cuda:0')}\n",
      "one epoch val: 142.3975522518158\n",
      "++++++++++++++++++++ Train Epoch 5 ++++++++++++++++++++\n",
      "Training CE Loss: 5.32597\n",
      "one epoch train: 14.874328851699829\n",
      "++++++++++++++++++++ Valid Epoch 5 ++++++++++++++++++++\n",
      "{'Recall@20': 0.001123046875, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.000341796875, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 137.94473958015442\n",
      "++++++++++++++++++++ Train Epoch 6 ++++++++++++++++++++\n",
      "Training CE Loss: 4.77737\n",
      "one epoch train: 15.502075910568237\n",
      "++++++++++++++++++++ Valid Epoch 6 ++++++++++++++++++++\n",
      "{'Recall@20': 0.00146484375, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.00048828125, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 131.39062666893005\n",
      "++++++++++++++++++++ Train Epoch 7 ++++++++++++++++++++\n",
      "Training CE Loss: 4.42827\n",
      "one epoch train: 15.880747318267822\n",
      "++++++++++++++++++++ Valid Epoch 7 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0015680387150496244, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.000244140625, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 129.60390520095825\n",
      "++++++++++++++++++++ Train Epoch 8 ++++++++++++++++++++\n",
      "Training CE Loss: 4.17909\n",
      "one epoch train: 14.842457056045532\n",
      "++++++++++++++++++++ Valid Epoch 8 ++++++++++++++++++++\n",
      "{'Recall@20': 0.000927734375, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.0001953125, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 134.05736756324768\n",
      "++++++++++++++++++++ Train Epoch 9 ++++++++++++++++++++\n",
      "Training CE Loss: 3.99435\n",
      "one epoch train: 15.371092319488525\n",
      "++++++++++++++++++++ Valid Epoch 9 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0010769881075248123, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.0002957381075248122, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 138.73004412651062\n",
      "++++++++++++++++++++ Train Epoch 10 ++++++++++++++++++++\n",
      "Training CE Loss: 3.85822\n",
      "one epoch train: 15.41323208808899\n",
      "++++++++++++++++++++ Valid Epoch 10 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0011746443575248121, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.0003445662325248122, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 133.78630471229553\n",
      "++++++++++++++++++++ Train Epoch 11 ++++++++++++++++++++\n",
      "Training CE Loss: 3.73043\n",
      "one epoch train: 15.342226266860962\n",
      "++++++++++++++++++++ Valid Epoch 11 ++++++++++++++++++++\n",
      "{'Recall@20': 0.00107421875, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.0001953125, 'NDCG@5': tensor(8.3222e-05, device='cuda:0')}\n",
      "one epoch val: 139.67560172080994\n",
      "++++++++++++++++++++ Train Epoch 12 ++++++++++++++++++++\n",
      "Training CE Loss: 3.64228\n",
      "one epoch train: 14.866636037826538\n",
      "++++++++++++++++++++ Valid Epoch 12 ++++++++++++++++++++\n",
      "{'Recall@20': 0.00166015625, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.000244140625, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 140.53800654411316\n",
      "++++++++++++++++++++ Train Epoch 13 ++++++++++++++++++++\n",
      "Training CE Loss: 3.56876\n",
      "one epoch train: 15.316386699676514\n",
      "++++++++++++++++++++ Valid Epoch 13 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0012262418400496244, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.0001953125, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 145.5595109462738\n",
      "++++++++++++++++++++ Train Epoch 14 ++++++++++++++++++++\n",
      "Training CE Loss: 3.49612\n",
      "one epoch train: 15.82271409034729\n",
      "++++++++++++++++++++ Valid Epoch 14 ++++++++++++++++++++\n",
      "{'Recall@20': 0.001416015625, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.0001953125, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 160.79274082183838\n",
      "++++++++++++++++++++ Train Epoch 15 ++++++++++++++++++++\n",
      "Training CE Loss: 3.44049\n",
      "one epoch train: 14.87454867362976\n",
      "++++++++++++++++++++ Valid Epoch 15 ++++++++++++++++++++\n",
      "{'Recall@20': 0.001123046875, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.000439453125, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 160.20956993103027\n",
      "++++++++++++++++++++ Train Epoch 16 ++++++++++++++++++++\n",
      "Training CE Loss: 3.38321\n",
      "one epoch train: 15.365765571594238\n",
      "++++++++++++++++++++ Valid Epoch 16 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0013211287325248123, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.000439453125, 'NDCG@5': tensor(0.0003, device='cuda:0')}\n",
      "one epoch val: 132.42919445037842\n",
      "++++++++++++++++++++ Train Epoch 17 ++++++++++++++++++++\n",
      "Training CE Loss: 3.34152\n",
      "one epoch train: 15.784130573272705\n",
      "++++++++++++++++++++ Valid Epoch 17 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0011774137150496245, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.0003933943575248122, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 134.4210205078125\n",
      "++++++++++++++++++++ Train Epoch 18 ++++++++++++++++++++\n",
      "Training CE Loss: 3.30346\n",
      "one epoch train: 15.33302354812622\n",
      "++++++++++++++++++++ Valid Epoch 18 ++++++++++++++++++++\n",
      "{'Recall@20': 0.000634765625, 'NDCG@20': tensor(0.0002, device='cuda:0'), 'Recall@5': 0.000146484375, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 138.4530439376831\n",
      "++++++++++++++++++++ Train Epoch 19 ++++++++++++++++++++\n",
      "Training CE Loss: 3.26989\n",
      "one epoch train: 14.739617586135864\n",
      "++++++++++++++++++++ Valid Epoch 19 ++++++++++++++++++++\n",
      "{'Recall@20': 0.00126953125, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.00029296875, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 129.95483779907227\n",
      "++++++++++++++++++++ Train Epoch 20 ++++++++++++++++++++\n",
      "Training CE Loss: 3.23463\n",
      "one epoch train: 15.19663405418396\n",
      "++++++++++++++++++++ Valid Epoch 20 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0014215543400496245, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 9.765625e-05, 'NDCG@5': tensor(4.5443e-05, device='cuda:0')}\n",
      "one epoch val: 109.21018433570862\n",
      "train_time: 305.4712064266205\n",
      "val_time: 2759.0909428596497\n",
      "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
      "{'Recall@20': 0.00087890625, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.000146484375, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "print('============= Hyperparameter Tuning - cold_start =============')\n",
    "train_loader, val_loader, test_loader = get_data(args) #, user_noclick\n",
    "\n",
    "print('generate sample from SBR file')\n",
    "generate_sbr_sample()\n",
    "\n",
    "# change the learing rate\n",
    "args['lr']  = 0.0003\n",
    "\n",
    "##########################################################\n",
    "\n",
    "\n",
    "process_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d0e2e",
   "metadata": {},
   "source": [
    "## Results\n",
    "### After tuning hyper-parameters\n",
    "\n",
    "### we can summarize the evaluation results as:\n",
    "\n",
    "#### Recall@20 : 0.00087890625\n",
    "\n",
    "#### NDCG@20 :0.0003\n",
    "\n",
    "#### Recall@5 : 0.000146484375\n",
    "\n",
    "#### NDCG@5 : 0.0001\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52264453",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning - Trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ce46458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Hyperparameter Tuning 2 - cold_start =============\n",
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting user ids and item ids in DataFrame ==========\n",
      "========== Initialize Reset DataFrame Object ==========\n",
      "========== Resetting item ids in DataFrame ==========\n",
      "+++user_history+++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39583/39583 [00:00<00:00, 47862.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate sample from SBR file\n",
      "pretrain\n",
      "++++++++++++++++++++ Train Epoch 1 ++++++++++++++++++++\n",
      "Training CE Loss: 11.01364\n",
      "one epoch train: 15.302217721939087\n",
      "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0013211287325248123, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.00029296875, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 127.79891467094421\n",
      "++++++++++++++++++++ Train Epoch 2 ++++++++++++++++++++\n",
      "Training CE Loss: 8.12589\n",
      "one epoch train: 14.78894305229187\n",
      "++++++++++++++++++++ Valid Epoch 2 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0008328474825248123, 'NDCG@20': tensor(0.0002, device='cuda:0'), 'Recall@5': 9.765625e-05, 'NDCG@5': tensor(5.5221e-05, device='cuda:0')}\n",
      "one epoch val: 126.32337927818298\n",
      "++++++++++++++++++++ Train Epoch 3 ++++++++++++++++++++\n",
      "Training CE Loss: 6.33734\n",
      "one epoch train: 15.282990217208862\n",
      "++++++++++++++++++++ Valid Epoch 3 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0011258162325248123, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.000244140625, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 129.4177439212799\n",
      "++++++++++++++++++++ Train Epoch 4 ++++++++++++++++++++\n",
      "Training CE Loss: 5.60048\n",
      "one epoch train: 14.788371086120605\n",
      "++++++++++++++++++++ Valid Epoch 4 ++++++++++++++++++++\n",
      "{'Recall@20': 0.001123046875, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.000244140625, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 127.6178617477417\n",
      "++++++++++++++++++++ Train Epoch 5 ++++++++++++++++++++\n",
      "Training CE Loss: 5.14841\n",
      "one epoch train: 15.921201944351196\n",
      "++++++++++++++++++++ Valid Epoch 5 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0010769881075248123, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.000341796875, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 124.54184579849243\n",
      "++++++++++++++++++++ Train Epoch 6 ++++++++++++++++++++\n",
      "Training CE Loss: 4.82827\n",
      "one epoch train: 15.57596206665039\n",
      "++++++++++++++++++++ Valid Epoch 6 ++++++++++++++++++++\n",
      "{'Recall@20': 0.00068359375, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.000146484375, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 137.5381317138672\n",
      "++++++++++++++++++++ Train Epoch 7 ++++++++++++++++++++\n",
      "Training CE Loss: 4.59419\n",
      "one epoch train: 15.466516971588135\n",
      "++++++++++++++++++++ Valid Epoch 7 ++++++++++++++++++++\n",
      "{'Recall@20': 0.001220703125, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 9.765625e-05, 'NDCG@5': tensor(7.9635e-05, device='cuda:0')}\n",
      "one epoch val: 144.35314202308655\n",
      "++++++++++++++++++++ Train Epoch 8 ++++++++++++++++++++\n",
      "Training CE Loss: 4.42412\n",
      "one epoch train: 14.80356740951538\n",
      "++++++++++++++++++++ Valid Epoch 8 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0013699568575248123, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.000390625, 'NDCG@5': tensor(0.0003, device='cuda:0')}\n",
      "one epoch val: 126.47597336769104\n",
      "++++++++++++++++++++ Train Epoch 9 ++++++++++++++++++++\n",
      "Training CE Loss: 4.27807\n",
      "one epoch train: 14.732890844345093\n",
      "++++++++++++++++++++ Valid Epoch 9 ++++++++++++++++++++\n",
      "{'Recall@20': 0.00126953125, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.000146484375, 'NDCG@5': tensor(9.2131e-05, device='cuda:0')}\n",
      "one epoch val: 110.6941442489624\n",
      "++++++++++++++++++++ Train Epoch 10 ++++++++++++++++++++\n",
      "Training CE Loss: 4.16637\n",
      "one epoch train: 15.303732633590698\n",
      "++++++++++++++++++++ Valid Epoch 10 ++++++++++++++++++++\n",
      "{'Recall@20': 0.00146484375, 'NDCG@20': tensor(0.0006, device='cuda:0'), 'Recall@5': 0.00068359375, 'NDCG@5': tensor(0.0004, device='cuda:0')}\n",
      "one epoch val: 111.07693696022034\n",
      "++++++++++++++++++++ Train Epoch 11 ++++++++++++++++++++\n",
      "Training CE Loss: 4.07700\n",
      "one epoch train: 15.28271198272705\n",
      "++++++++++++++++++++ Valid Epoch 11 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0016684642992913722, 'NDCG@20': tensor(0.0006, device='cuda:0'), 'Recall@5': 0.00029296875, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 113.80641794204712\n",
      "++++++++++++++++++++ Train Epoch 12 ++++++++++++++++++++\n",
      "Training CE Loss: 4.00223\n",
      "one epoch train: 15.038484334945679\n",
      "++++++++++++++++++++ Valid Epoch 12 ++++++++++++++++++++\n",
      "{'Recall@20': 0.00107421875, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.0005859375, 'NDCG@5': tensor(0.0003, device='cuda:0')}\n",
      "one epoch val: 110.36277532577515\n",
      "++++++++++++++++++++ Train Epoch 13 ++++++++++++++++++++\n",
      "Training CE Loss: 3.94707\n",
      "one epoch train: 15.56613302230835\n",
      "++++++++++++++++++++ Valid Epoch 13 ++++++++++++++++++++\n",
      "{'Recall@20': 0.001806640625, 'NDCG@20': tensor(0.0006, device='cuda:0'), 'Recall@5': 0.000244140625, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 107.93768119812012\n",
      "++++++++++++++++++++ Train Epoch 14 ++++++++++++++++++++\n",
      "Training CE Loss: 3.88956\n",
      "one epoch train: 14.952369213104248\n",
      "++++++++++++++++++++ Valid Epoch 14 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0012723006075248122, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.000244140625, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 105.14127039909363\n",
      "++++++++++++++++++++ Train Epoch 15 ++++++++++++++++++++\n",
      "Training CE Loss: 3.84991\n",
      "one epoch train: 15.334986209869385\n",
      "++++++++++++++++++++ Valid Epoch 15 ++++++++++++++++++++\n",
      "{'Recall@20': 0.00078125, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.000146484375, 'NDCG@5': tensor(8.6607e-05, device='cuda:0')}\n",
      "one epoch val: 112.75532484054565\n",
      "++++++++++++++++++++ Train Epoch 16 ++++++++++++++++++++\n",
      "Training CE Loss: 3.81035\n",
      "one epoch train: 15.87763237953186\n",
      "++++++++++++++++++++ Valid Epoch 16 ++++++++++++++++++++\n",
      "{'Recall@20': 0.001318359375, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.000537109375, 'NDCG@5': tensor(0.0003, device='cuda:0')}\n",
      "one epoch val: 114.92184448242188\n",
      "++++++++++++++++++++ Train Epoch 17 ++++++++++++++++++++\n",
      "Training CE Loss: 3.78186\n",
      "one epoch train: 14.942619323730469\n",
      "++++++++++++++++++++ Valid Epoch 17 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0015164412325248122, 'NDCG@20': tensor(0.0005, device='cuda:0'), 'Recall@5': 0.000341796875, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n",
      "one epoch val: 110.76786279678345\n",
      "++++++++++++++++++++ Train Epoch 18 ++++++++++++++++++++\n",
      "Training CE Loss: 3.75342\n",
      "one epoch train: 15.327032327651978\n",
      "++++++++++++++++++++ Valid Epoch 18 ++++++++++++++++++++\n",
      "{'Recall@20': 0.000830078125, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 9.765625e-05, 'NDCG@5': tensor(6.9857e-05, device='cuda:0')}\n",
      "one epoch val: 120.86014246940613\n",
      "++++++++++++++++++++ Train Epoch 19 ++++++++++++++++++++\n",
      "Training CE Loss: 3.73196\n",
      "one epoch train: 15.924956798553467\n",
      "++++++++++++++++++++ Valid Epoch 19 ++++++++++++++++++++\n",
      "{'Recall@20': 0.000927734375, 'NDCG@20': tensor(0.0003, device='cuda:0'), 'Recall@5': 0.000244140625, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 120.98521947860718\n",
      "++++++++++++++++++++ Train Epoch 20 ++++++++++++++++++++\n",
      "Training CE Loss: 3.71019\n",
      "one epoch train: 15.341031312942505\n",
      "++++++++++++++++++++ Valid Epoch 20 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0011746443575248121, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.0002957381075248122, 'NDCG@5': tensor(0.0001, device='cuda:0')}\n",
      "one epoch val: 122.46685886383057\n",
      "train_time: 305.5543508529663\n",
      "val_time: 2405.8434715270996\n",
      "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
      "{'Recall@20': 0.0010277157765813173, 'NDCG@20': tensor(0.0004, device='cuda:0'), 'Recall@5': 0.000244140625, 'NDCG@5': tensor(0.0002, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "print('============= Hyperparameter Tuning 2 - cold_start =============')\n",
    "train_loader, val_loader, test_loader = get_data(args) #, user_noclick\n",
    "\n",
    "print('generate sample from SBR file')\n",
    "generate_sbr_sample()\n",
    "\n",
    "# change learning rate and patch size\n",
    "\n",
    "args['train_batch_size'] = 2048\n",
    "\n",
    "args['val_batch_size'] = 2048\n",
    "\n",
    "args['test_batch_size'] = 2048\n",
    "\n",
    "args['lr']  = 0.0008\n",
    "\n",
    "##########################################################\n",
    "\n",
    "\n",
    "process_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25c9f8",
   "metadata": {},
   "source": [
    "## Results\n",
    "### After tuning hyper-parameters\n",
    "\n",
    "### we can summarize the evaluation results as:\n",
    "\n",
    "#### Recall@20 : 0.0010277157765813173\n",
    "\n",
    "#### NDCG@20 : 0.0004\n",
    "\n",
    "#### Recall@5 : 0.000244140625\n",
    "\n",
    "#### NDCG@5 : 0.0002\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e788ff5c",
   "metadata": {},
   "source": [
    "# After tuning hyper-parameters we can say that the performance became better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fabf563",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://github.com/yuangh-x/2022-NIPS-Tenrec\n",
    "\n",
    "https://tenrec0.github.io/assets/doc/NIPS2022-Tenrec.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
